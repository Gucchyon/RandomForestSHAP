{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwMdPHP3D5zZBUM5YjzSWS",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gucchyon/RandomForestSHAP/blob/main/BiocharRandomForestSHAP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#データセットの準備"
      ],
      "metadata": {
        "id": "Ovg7-uUKvkHi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Myp1CJKF-39P",
        "outputId": "f12f08a2-c76f-46af-c7a6-4ae1b82a20f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (2.1.4)\n",
            "Requirement already satisfied: shap in /usr/local/lib/python3.11/dist-packages (0.47.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Collecting scikeras\n",
            "  Downloading scikeras-0.13.0-py3-none-any.whl.metadata (3.1 kB)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.11/dist-packages (from shap) (4.67.1)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.11/dist-packages (from shap) (24.2)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.11/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba>=0.54 in /usr/local/lib/python3.11/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from shap) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.11/dist-packages (from shap) (4.12.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.1)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.1.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.14.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.54->shap) (0.43.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading scikeras-0.13.0-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: scikeras\n",
            "Successfully installed scikeras-0.13.0\n",
            "Created folder: section1_data_preparation\n",
            "Created folder: section2_model_comparison\n",
            "Created folder: section3_final_model\n",
            "Created folder: section4_predictions_analysis\n",
            "Created folder: section5_shap_analysis\n",
            "Created folder: section6_interaction_analysis\n",
            "Created folder: section7_feature_pdp\n",
            "Created folder: section1_data_preparation/histograms\n",
            "Created folder: section4_predictions_analysis/extreme_values\n",
            "ファイル「BiocharDS_V1.0.xlsx」が見つかりません。アップロードしてください。\n",
            "ファイル「BiocharDS_V1.0.xlsx」をアップロードしてください。Sheet1とExplanation and unitの2つのシートが必要です。\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-6e0e2301-1c31-41ea-bb83-28101e2c111e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-6e0e2301-1c31-41ea-bb83-28101e2c111e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving BiocharDS_V1.0.xlsx to BiocharDS_V1.0.xlsx\n",
            "ファイル「BiocharDS_V1.0.xlsx」が正常にアップロードされました。\n",
            "元のデータサイズ: (2438, 209)\n",
            "イネのデータサイズ: (327, 209)\n",
            "目的変数作成後のデータサイズ: (327, 210)\n",
            "目的変数が欠損しているインスタンス数: 40\n",
            "NaN除去前のデータ数: 327\n",
            "NaN除去後のデータ数: 287\n",
            "NaN除去により削減されたデータ数: 40\n",
            "Starting fixed threshold filtering process...\n",
            "フィルタリング前のデータ数: 287\n",
            "フィルタリング後のデータ数: 285\n",
            "除外されたデータ数: 2件 (0.70%)\n",
            "目的変数のフィルタリング前のデータ数: 287\n",
            "目的変数のフィルタリング後のデータ数: 285\n",
            "目的変数のフィルタリングにより削減されたデータ数: 2\n",
            "最終的なデータセットサイズ: 285\n",
            "\n",
            "================ データ処理の総括 ================\n",
            "1. 元のイネデータ数: 327\n",
            "2. 目的変数欠損による除外: 40\n",
            "3. 目的変数のフィルタリングによる除外: 2\n",
            "   - 除外基準: 目的変数が-3以下または5以上のデータ\n",
            "4. 最終データ数: 285\n",
            "総削減数: 42\n",
            "================================================\n",
            "\n",
            "\n",
            "=== バイオ炭タイプの変換結果 ===\n",
            "1. 木質系バイオマス (Wood residues): 8件\n",
            "2. わら系残渣 (Straw residues): 230件\n",
            "3. 畜産廃棄物 (Livestock manure): 10件\n",
            "4. 殻系残渣 (Shell residues): 36件\n",
            "未分類/欠損値: 1件\n",
            "Missing data percentage for each feature (%):\n",
            "Temperature: 15.09%\n",
            "Precipitation: 8.77%\n",
            "SOC_initial: 14.74%\n",
            "TN_initial: 10.18%\n",
            "TP_initial: 72.28%\n",
            "NH4_N_initial: 91.58%\n",
            "NO3_N_initial: 96.49%\n",
            "OlsenP_initial: 64.56%\n",
            "pH_initial: 10.18%\n",
            "BD_initial: 64.21%\n",
            "CEC_initial: 69.47%\n",
            "Texture_initial: 45.96%\n",
            "Type_biochar: 0.35%\n",
            "ParticleSize_biochar: 72.28%\n",
            "BD_biochar: 89.47%\n",
            "PyrolysisTemperature_biochar: 7.02%\n",
            "Ash_biochar: 82.46%\n",
            "C_biochar: 12.28%\n",
            "N_biochar: 7.02%\n",
            "P_biochar: 47.02%\n",
            "CNRatio_biochar: 12.63%\n",
            "pH_biochar: 12.98%\n",
            "CEC_biochar: 63.51%\n",
            "BiocharAddition: 0.00%\n",
            "TrialDuration: 26.67%\n",
            "\n",
            "Excluding features with missing data ≥ 50%: ['TP_initial', 'NH4_N_initial', 'NO3_N_initial', 'OlsenP_initial', 'BD_initial', 'CEC_initial', 'ParticleSize_biochar', 'BD_biochar', 'Ash_biochar', 'CEC_biochar']\n",
            "\n",
            "Number of missing values in explanatory variables after processing: 658\n",
            "X_original index shape: (285,)\n",
            "X_processed index shape: (285,)\n",
            "共通するインデックスの数: 285\n",
            "\n",
            "補間後の説明変数と目的変数のヒストグラムを作成中...\n",
            "ヒストグラムを section1_data_preparation/histograms フォルダに保存しました。\n",
            "データに含まれる国のリスト: ['China' 'Italy' 'Korea' 'Iran' 'Bangladesh' 'Malaysia' 'Philippines'\n",
            " 'Thailand' 'India' 'Brazil']\n",
            "Chinaのデータ数: 251\n",
            "Italyのデータ数: 2\n",
            "Koreaのデータ数: 3\n",
            "Iranのデータ数: 4\n",
            "Bangladeshのデータ数: 2\n",
            "Malaysiaのデータ数: 1\n",
            "Philippinesのデータ数: 12\n",
            "Thailandのデータ数: 6\n",
            "Indiaのデータ数: 3\n",
            "Brazilのデータ数: 1\n",
            "\n",
            "======= 5分割交差検証によるモデル評価（シンプル版） =======\n",
            "\n",
            "PLSRの評価中...\n",
            "  パラメータ評価中: n_components=1\n",
            "    RMSE: 1.0725, R²: 0.0135\n",
            "  パラメータ評価中: n_components=3\n",
            "    RMSE: 1.1002, R²: -0.0381\n",
            "  パラメータ評価中: n_components=5\n",
            "    RMSE: 1.1115, R²: -0.0594\n",
            "  パラメータ評価中: n_components=10\n",
            "    RMSE: 1.1213, R²: -0.0782\n",
            "  パラメータ評価中: n_components=15\n",
            "    RMSE: 1.1217, R²: -0.0791\n",
            "\n",
            "最良のパラメータ: {'n_components': 1}\n",
            "CV R²: 0.0135, CV RMSE: 1.0725, CV RRMSE: 107.15%\n",
            "\n",
            "Decision Treeの評価中...\n",
            "  パラメータ評価中: max_depth=5, min_samples_split=2\n",
            "    RMSE: 0.9099, R²: 0.2899\n",
            "  パラメータ評価中: max_depth=5, min_samples_split=5\n",
            "    RMSE: 0.9202, R²: 0.2738\n",
            "  パラメータ評価中: max_depth=5, min_samples_split=10\n",
            "    RMSE: 0.9235, R²: 0.2686\n",
            "  パラメータ評価中: max_depth=10, min_samples_split=2\n",
            "    RMSE: 0.9503, R²: 0.2256\n",
            "  パラメータ評価中: max_depth=10, min_samples_split=5\n",
            "    RMSE: 0.9115, R²: 0.2876\n",
            "  パラメータ評価中: max_depth=10, min_samples_split=10\n",
            "    RMSE: 0.8984, R²: 0.3079\n",
            "  パラメータ評価中: max_depth=15, min_samples_split=2\n",
            "    RMSE: 0.9430, R²: 0.2373\n",
            "  パラメータ評価中: max_depth=15, min_samples_split=5\n",
            "    RMSE: 0.9169, R²: 0.2790\n",
            "  パラメータ評価中: max_depth=15, min_samples_split=10\n",
            "    RMSE: 0.8902, R²: 0.3204\n",
            "  パラメータ評価中: max_depth=None, min_samples_split=2\n",
            "    RMSE: 0.9617, R²: 0.2069\n",
            "  パラメータ評価中: max_depth=None, min_samples_split=5\n",
            "    RMSE: 0.9169, R²: 0.2790\n",
            "  パラメータ評価中: max_depth=None, min_samples_split=10\n",
            "    RMSE: 0.8902, R²: 0.3204\n",
            "\n",
            "最良のパラメータ: {'max_depth': 15, 'min_samples_split': 10}\n",
            "CV R²: 0.3204, CV RMSE: 0.8902, CV RRMSE: 88.94%\n",
            "\n",
            "Random Forestの評価中...\n",
            "  パラメータ評価中: n_estimators=50, max_depth=10\n",
            "    RMSE: 0.7581, R²: 0.5071\n",
            "  パラメータ評価中: n_estimators=50, max_depth=20\n",
            "    RMSE: 0.7630, R²: 0.5008\n",
            "  パラメータ評価中: n_estimators=50, max_depth=None\n",
            "    RMSE: 0.7630, R²: 0.5008\n",
            "  パラメータ評価中: n_estimators=100, max_depth=10\n",
            "    RMSE: 0.7538, R²: 0.5127\n",
            "  パラメータ評価中: n_estimators=100, max_depth=20\n",
            "    RMSE: 0.7586, R²: 0.5065\n",
            "  パラメータ評価中: n_estimators=100, max_depth=None\n",
            "    RMSE: 0.7586, R²: 0.5065\n",
            "  パラメータ評価中: n_estimators=200, max_depth=10\n",
            "    RMSE: 0.7525, R²: 0.5144\n",
            "  パラメータ評価中: n_estimators=200, max_depth=20\n",
            "    RMSE: 0.7527, R²: 0.5142\n",
            "  パラメータ評価中: n_estimators=200, max_depth=None\n",
            "    RMSE: 0.7527, R²: 0.5142\n",
            "\n",
            "最良のパラメータ: {'n_estimators': 200, 'max_depth': 10}\n",
            "CV R²: 0.5144, CV RMSE: 0.7525, CV RRMSE: 75.18%\n",
            "\n",
            "XGBoostの評価中...\n",
            "  パラメータ評価中: n_estimators=50, max_depth=3, learning_rate=0.01\n",
            "    RMSE: 0.9576, R²: 0.2136\n",
            "  パラメータ評価中: n_estimators=50, max_depth=3, learning_rate=0.1\n",
            "    RMSE: 0.7709, R²: 0.4904\n",
            "  パラメータ評価中: n_estimators=50, max_depth=3, learning_rate=0.2\n",
            "    RMSE: 0.7177, R²: 0.5583\n",
            "  パラメータ評価中: n_estimators=50, max_depth=5, learning_rate=0.01\n",
            "    RMSE: 0.9018, R²: 0.3025\n",
            "  パラメータ評価中: n_estimators=50, max_depth=5, learning_rate=0.1\n",
            "    RMSE: 0.7419, R²: 0.5279\n",
            "  パラメータ評価中: n_estimators=50, max_depth=5, learning_rate=0.2\n",
            "    RMSE: 0.7621, R²: 0.5019\n",
            "  パラメータ評価中: n_estimators=50, max_depth=7, learning_rate=0.01\n",
            "    RMSE: 0.8917, R²: 0.3181\n",
            "  パラメータ評価中: n_estimators=50, max_depth=7, learning_rate=0.1\n",
            "    RMSE: 0.7794, R²: 0.4790\n",
            "  パラメータ評価中: n_estimators=50, max_depth=7, learning_rate=0.2\n",
            "    RMSE: 0.8109, R²: 0.4361\n",
            "  パラメータ評価中: n_estimators=100, max_depth=3, learning_rate=0.01\n",
            "    RMSE: 0.8894, R²: 0.3217\n",
            "  パラメータ評価中: n_estimators=100, max_depth=3, learning_rate=0.1\n",
            "    RMSE: 0.7407, R²: 0.5295\n",
            "  パラメータ評価中: n_estimators=100, max_depth=3, learning_rate=0.2\n",
            "    RMSE: 0.7043, R²: 0.5746\n",
            "  パラメータ評価中: n_estimators=100, max_depth=5, learning_rate=0.01\n",
            "    RMSE: 0.8190, R²: 0.4248\n",
            "  パラメータ評価中: n_estimators=100, max_depth=5, learning_rate=0.1\n",
            "    RMSE: 0.7486, R²: 0.5194\n",
            "  パラメータ評価中: n_estimators=100, max_depth=5, learning_rate=0.2\n",
            "    RMSE: 0.7826, R²: 0.4748\n",
            "  パラメータ評価中: n_estimators=100, max_depth=7, learning_rate=0.01\n",
            "    RMSE: 0.8159, R²: 0.4291\n",
            "  パラメータ評価中: n_estimators=100, max_depth=7, learning_rate=0.1\n",
            "    RMSE: 0.7976, R²: 0.4544\n",
            "  パラメータ評価中: n_estimators=100, max_depth=7, learning_rate=0.2\n",
            "    RMSE: 0.8208, R²: 0.4223\n",
            "  パラメータ評価中: n_estimators=200, max_depth=3, learning_rate=0.01\n",
            "    RMSE: 0.8228, R²: 0.4194\n",
            "  パラメータ評価中: n_estimators=200, max_depth=3, learning_rate=0.1\n",
            "    RMSE: 0.7128, R²: 0.5643\n",
            "  パラメータ評価中: n_estimators=200, max_depth=3, learning_rate=0.2\n",
            "    RMSE: 0.7165, R²: 0.5598\n",
            "  パラメータ評価中: n_estimators=200, max_depth=5, learning_rate=0.01\n",
            "    RMSE: 0.7698, R²: 0.4919\n",
            "  パラメータ評価中: n_estimators=200, max_depth=5, learning_rate=0.1\n",
            "    RMSE: 0.7762, R²: 0.4833\n",
            "  パラメータ評価中: n_estimators=200, max_depth=5, learning_rate=0.2\n",
            "    RMSE: 0.7943, R²: 0.4589\n",
            "  パラメータ評価中: n_estimators=200, max_depth=7, learning_rate=0.01\n",
            "    RMSE: 0.7853, R²: 0.4711\n",
            "  パラメータ評価中: n_estimators=200, max_depth=7, learning_rate=0.1\n",
            "    RMSE: 0.8116, R²: 0.4351\n",
            "  パラメータ評価中: n_estimators=200, max_depth=7, learning_rate=0.2\n",
            "    RMSE: 0.8211, R²: 0.4218\n",
            "\n",
            "最良のパラメータ: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.2}\n",
            "CV R²: 0.5746, CV RMSE: 0.7043, CV RRMSE: 70.36%\n",
            "\n",
            "Neural Networkの評価中...\n",
            "  パラメータ評価中: layers=3, units=32, dropout=0.2, lr=0.001, batch_size=8\n",
            "  フォールド 1/5 処理中...\n",
            "  フォールド 2/5 処理中...\n",
            "  フォールド 3/5 処理中...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x79e5b2591260> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x79e5b2591260> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  フォールド 4/5 処理中...\n",
            "  フォールド 5/5 処理中...\n",
            "    RMSE: 0.9838, R²: 0.1701\n",
            "  パラメータ評価中: layers=3, units=32, dropout=0.2, lr=0.001, batch_size=16\n",
            "  フォールド 1/5 処理中...\n",
            "  フォールド 2/5 処理中...\n",
            "  フォールド 3/5 処理中...\n",
            "  フォールド 4/5 処理中...\n",
            "  フォールド 5/5 処理中...\n",
            "    RMSE: 1.0944, R²: -0.0270\n",
            "  パラメータ評価中: layers=3, units=32, dropout=0.2, lr=0.001, batch_size=32\n",
            "  フォールド 1/5 処理中...\n",
            "  フォールド 2/5 処理中...\n",
            "  フォールド 3/5 処理中...\n",
            "  フォールド 4/5 処理中...\n",
            "  フォールド 5/5 処理中...\n",
            "    RMSE: 1.0369, R²: 0.0779\n",
            "  パラメータ評価中: layers=3, units=32, dropout=0.2, lr=0.01, batch_size=8\n",
            "  フォールド 1/5 処理中...\n",
            "  フォールド 2/5 処理中...\n",
            "  フォールド 3/5 処理中...\n",
            "  フォールド 4/5 処理中...\n",
            "  フォールド 5/5 処理中...\n",
            "    RMSE: 0.9722, R²: 0.1895\n",
            "  パラメータ評価中: layers=3, units=32, dropout=0.2, lr=0.01, batch_size=16\n",
            "  フォールド 1/5 処理中...\n",
            "  フォールド 2/5 処理中...\n",
            "  フォールド 3/5 処理中...\n",
            "  フォールド 4/5 処理中...\n",
            "  フォールド 5/5 処理中...\n",
            "    RMSE: 0.9812, R²: 0.1744\n",
            "  パラメータ評価中: layers=3, units=32, dropout=0.2, lr=0.01, batch_size=32\n",
            "  フォールド 1/5 処理中...\n",
            "  フォールド 2/5 処理中...\n",
            "  フォールド 3/5 処理中...\n",
            "  フォールド 4/5 処理中...\n",
            "  フォールド 5/5 処理中...\n",
            "    RMSE: 1.1129, R²: -0.0620\n",
            "  パラメータ評価中: layers=3, units=32, dropout=0.2, lr=0.1, batch_size=8\n",
            "  フォールド 1/5 処理中...\n",
            "  フォールド 2/5 処理中...\n",
            "  フォールド 3/5 処理中...\n",
            "  フォールド 4/5 処理中...\n",
            "  フォールド 5/5 処理中...\n",
            "    RMSE: 1.0367, R²: 0.0784\n",
            "  パラメータ評価中: layers=3, units=32, dropout=0.2, lr=0.1, batch_size=16\n",
            "  フォールド 1/5 処理中...\n",
            "  フォールド 2/5 処理中...\n",
            "  フォールド 3/5 処理中...\n",
            "  フォールド 4/5 処理中...\n",
            "  フォールド 5/5 処理中...\n",
            "    RMSE: 1.0173, R²: 0.1126\n",
            "  パラメータ評価中: layers=3, units=32, dropout=0.2, lr=0.1, batch_size=32\n",
            "  フォールド 1/5 処理中...\n",
            "  フォールド 2/5 処理中...\n",
            "  フォールド 3/5 処理中...\n",
            "  フォールド 4/5 処理中...\n",
            "  フォールド 5/5 処理中...\n",
            "    RMSE: 1.1192, R²: -0.0742\n",
            "\n",
            "最良のパラメータ: {'layers': 3, 'units': 32, 'dropout': 0.2, 'learning_rate': 0.01, 'batch_size': 8}\n",
            "CV R²: 0.1895, CV RMSE: 0.9722, CV RRMSE: 97.13%\n",
            "\n",
            "交差検証による比較結果:\n",
            "        Algorithm                                        Best_Params  \\\n",
            "3         XGBoost  {'n_estimators': 100, 'max_depth': 3, 'learnin...   \n",
            "2   Random Forest             {'n_estimators': 200, 'max_depth': 10}   \n",
            "1   Decision Tree         {'max_depth': 15, 'min_samples_split': 10}   \n",
            "4  Neural Network  {'layers': 3, 'units': 32, 'dropout': 0.2, 'le...   \n",
            "0            PLSR                                {'n_components': 1}   \n",
            "\n",
            "      CV_R2   CV_RMSE    CV_RRMSE  \n",
            "3  0.574588  0.704320   70.364827  \n",
            "2  0.514418  0.752482   75.176453  \n",
            "1  0.320407  0.890203   88.935459  \n",
            "4  0.189471  0.972185   97.125865  \n",
            "0  0.013482  1.072549  107.152685  \n",
            "\n",
            "ランダムフォレストの最終モデル構築中...\n",
            "セクション2から取得した最適パラメータ: {'n_estimators': 200, 'max_depth': 10}\n",
            "訓練データでの決定係数 (R²): 0.8808\n",
            "訓練データでのRMSE: 0.3729\n",
            "訓練データでのRRMSE: 37.2507%\n"
          ]
        }
      ],
      "source": [
        "# ============= １．データセットの準備 =============\n",
        "\n",
        "# 必要なライブラリのインストール\n",
        "!pip install scikit-learn xgboost shap pandas numpy matplotlib seaborn openpyxl tensorflow scikeras\n",
        "\n",
        "# ライブラリのインポート\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import shap\n",
        "import warnings\n",
        "import os\n",
        "from google.colab import files\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import KNNImputer\n",
        "from sklearn.model_selection import KFold, GridSearchCV\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "# scikerasをインポート（TensorFlowの新しいバージョンではtensorflow.keras.wrappers.scikit_learnが廃止されたため）\n",
        "from scikeras.wrappers import KerasRegressor\n",
        "\n",
        "# Create main folders for each section\n",
        "sections = [\n",
        "    \"section1_data_preparation\",\n",
        "    \"section2_model_comparison\",\n",
        "    \"section3_final_model\",\n",
        "    \"section4_predictions_analysis\",\n",
        "    \"section5_shap_analysis\",\n",
        "    \"section6_interaction_analysis\",\n",
        "    \"section7_feature_pdp\"\n",
        "]\n",
        "\n",
        "# Create each folder\n",
        "for section in sections:\n",
        "    if not os.path.exists(section):\n",
        "        os.makedirs(section)\n",
        "        print(f\"Created folder: {section}\")\n",
        "\n",
        "# Create subfolder for histograms in section 1\n",
        "histograms_dir = os.path.join(\"section1_data_preparation\", \"histograms\")\n",
        "if not os.path.exists(histograms_dir):\n",
        "    os.makedirs(histograms_dir)\n",
        "    print(f\"Created folder: {histograms_dir}\")\n",
        "\n",
        "# Create subfolder for extreme values analysis in section 4\n",
        "extremes_dir = os.path.join(\"section4_predictions_analysis\", \"extreme_values\")\n",
        "if not os.path.exists(extremes_dir):\n",
        "    os.makedirs(extremes_dir)\n",
        "    print(f\"Created folder: {extremes_dir}\")\n",
        "\n",
        "# ランダムシードの設定\n",
        "np.random.seed(42)\n",
        "\n",
        "# 警告を無視\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# BiocharDS_V1.0.xlsxファイルが存在するか確認\n",
        "try:\n",
        "    # 既存ファイルを確認\n",
        "    biochar_data = pd.read_excel('BiocharDS_V1.0.xlsx', sheet_name='Sheet1')\n",
        "    explanation_data = pd.read_excel('BiocharDS_V1.0.xlsx', sheet_name='Explanation and unit')\n",
        "    print(\"ファイル「BiocharDS_V1.0.xlsx」が正常に読み込まれました。\")\n",
        "\n",
        "except FileNotFoundError:\n",
        "    # ファイルがない場合はアップロードを促す\n",
        "    print(\"ファイル「BiocharDS_V1.0.xlsx」が見つかりません。アップロードしてください。\")\n",
        "    print(\"ファイル「BiocharDS_V1.0.xlsx」をアップロードしてください。Sheet1とExplanation and unitの2つのシートが必要です。\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    # アップロードされたファイルを確認\n",
        "    if 'BiocharDS_V1.0.xlsx' in uploaded:\n",
        "        biochar_data = pd.read_excel('BiocharDS_V1.0.xlsx', sheet_name='Sheet1')\n",
        "        explanation_data = pd.read_excel('BiocharDS_V1.0.xlsx', sheet_name='Explanation and unit')\n",
        "        print(\"ファイル「BiocharDS_V1.0.xlsx」が正常にアップロードされました。\")\n",
        "    else:\n",
        "        raise FileNotFoundError(\"必要なファイル「BiocharDS_V1.0.xlsx」がアップロードされませんでした。\")\n",
        "\n",
        "# 列名のスペースを削除\n",
        "biochar_data.columns = [col.strip() for col in biochar_data.columns]\n",
        "explanation_data.columns = [col.strip() for col in explanation_data.columns]\n",
        "\n",
        "# イネ（Rice）のデータのみを選択\n",
        "rice_data = biochar_data[(biochar_data['CropType'] == 'Rice') & (biochar_data['Treatment'] == 'B')].copy()\n",
        "\n",
        "# カウント情報を表示\n",
        "print(f\"元のデータサイズ: {biochar_data.shape}\")\n",
        "print(f\"イネのデータサイズ: {rice_data.shape}\")\n",
        "\n",
        "# 目的変数の作成: CropYield_T - CropYield_CK\n",
        "rice_data['Yield_Difference'] = rice_data['CropYield_T'] - rice_data['CropYield_CK']\n",
        "\n",
        "# 目的変数作成後のデータサイズを確認\n",
        "print(f\"目的変数作成後のデータサイズ: {rice_data.shape}\")\n",
        "\n",
        "# 目的変数が欠損しているインスタンスの数を確認\n",
        "missing_target_count = rice_data['Yield_Difference'].isna().sum()\n",
        "print(f\"目的変数が欠損しているインスタンス数: {missing_target_count}\")\n",
        "\n",
        "# 目的変数が欠損しているインスタンスを除去する前のデータ数を記録\n",
        "before_dropna_count = len(rice_data)\n",
        "\n",
        "# 目的変数が欠損しているインスタンスを除去\n",
        "rice_data = rice_data.dropna(subset=['Yield_Difference'])\n",
        "\n",
        "# 欠損値除去後のデータ数を記録\n",
        "after_dropna_count = len(rice_data)\n",
        "dropped_na_count = before_dropna_count - after_dropna_count\n",
        "\n",
        "print(f\"NaN除去前のデータ数: {before_dropna_count}\")\n",
        "print(f\"NaN除去後のデータ数: {after_dropna_count}\")\n",
        "print(f\"NaN除去により削減されたデータ数: {dropped_na_count}\")\n",
        "\n",
        "# 外れ値除去: 目的変数の値域に基づくフィルタリング\n",
        "print(\"Starting fixed threshold filtering process...\")\n",
        "\n",
        "# フィルタリング前のデータ数を記録\n",
        "before_filtering_count = len(rice_data)\n",
        "\n",
        "# 目的変数の値が-3以下あるいは5以上のデータを除去\n",
        "filtered_rice_data = rice_data[(rice_data['Yield_Difference'] > -3) & (rice_data['Yield_Difference'] < 8)]\n",
        "\n",
        "# 除外されたデータ数を計算\n",
        "excluded_count = before_filtering_count - len(filtered_rice_data)\n",
        "excluded_pct = (excluded_count / before_filtering_count) * 100\n",
        "\n",
        "# 結果を表示\n",
        "print(f\"フィルタリング前のデータ数: {before_filtering_count}\")\n",
        "print(f\"フィルタリング後のデータ数: {len(filtered_rice_data)}\")\n",
        "print(f\"除外されたデータ数: {excluded_count}件 ({excluded_pct:.2f}%)\")\n",
        "\n",
        "# 元の変数に代入\n",
        "rice_data = filtered_rice_data\n",
        "\n",
        "# 目的変数のフィルタリング後のデータ数を記録\n",
        "after_filtering_count = len(rice_data)\n",
        "filtered_count = before_filtering_count - after_filtering_count\n",
        "\n",
        "print(f\"目的変数のフィルタリング前のデータ数: {before_filtering_count}\")\n",
        "print(f\"目的変数のフィルタリング後のデータ数: {after_filtering_count}\")\n",
        "print(f\"目的変数のフィルタリングにより削減されたデータ数: {filtered_count}\")\n",
        "\n",
        "# 説明変数の選択\n",
        "features = [\n",
        "    'Temperature', 'Precipitation',#'WetnessIndex',\n",
        "    'SOC_initial', 'TN_initial', 'TP_initial',\n",
        "    'NH4_N_initial', 'NO3_N_initial', 'OlsenP_initial', 'pH_initial',\n",
        "    'BD_initial', 'CEC_initial', 'Texture_initial', 'Type_biochar',\n",
        "    'ParticleSize_biochar','BD_biochar', 'PyrolysisTemperature_biochar',\n",
        "    'Ash_biochar','C_biochar', 'N_biochar', 'P_biochar', 'CNRatio_biochar',\n",
        "    'pH_biochar', 'CEC_biochar',\n",
        "    'BiocharAddition', 'TrialDuration'\n",
        "]\n",
        "\n",
        "# 説明変数のコピーを作成\n",
        "all_features = features.copy()\n",
        "\n",
        "# 最終的なデータサイズの報告\n",
        "final_dataset_size = len(rice_data)\n",
        "print(f\"最終的なデータセットサイズ: {final_dataset_size}\")\n",
        "\n",
        "# データ処理の総括を表示\n",
        "print(\"\\n================ データ処理の総括 ================\")\n",
        "print(f\"1. 元のイネデータ数: {before_dropna_count}\")\n",
        "print(f\"2. 目的変数欠損による除外: {dropped_na_count}\")\n",
        "print(f\"3. 目的変数のフィルタリングによる除外: {filtered_count}\")\n",
        "print(f\"   - 除外基準: 目的変数が-3以下または5以上のデータ\")\n",
        "print(f\"4. 最終データ数: {final_dataset_size}\")\n",
        "print(f\"総削減数: {before_dropna_count - final_dataset_size}\")\n",
        "print(\"================================================\\n\")\n",
        "\n",
        "# フィルタリング後の目的変数の分布を可視化\n",
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "plt.subplot(2, 1, 1)\n",
        "plt.boxplot(rice_data['Yield_Difference'])\n",
        "plt.title('Distribution of Yield Difference After Filtering')\n",
        "plt.ylabel('Yield Difference (CropYield_T - CropYield_CK)')\n",
        "# 分布の範囲を明示\n",
        "plt.axhline(y=-3, color='r', linestyle='--', alpha=0.5)\n",
        "plt.axhline(y=4, color='r', linestyle='--', alpha=0.5)\n",
        "\n",
        "plt.subplot(2, 1, 2)\n",
        "plt.hist(rice_data['Yield_Difference'], bins=20)\n",
        "plt.title('Histogram of Yield Difference After Filtering')\n",
        "plt.xlabel('Yield Difference')\n",
        "plt.ylabel('Frequency')\n",
        "# ヒストグラムにも範囲の境界線を追加\n",
        "plt.axvline(x=-3, color='r', linestyle='--', alpha=0.5)\n",
        "plt.axvline(x=4, color='r', linestyle='--', alpha=0.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join('section1_data_preparation', 'Yield_Difference_Distribution_After_Filtering.png'))\n",
        "plt.close()\n",
        "\n",
        "# カラム名の修正（Silt _initialをSilt_initialに変更）\n",
        "if 'Silt _initial' in rice_data.columns:\n",
        "    rice_data = rice_data.rename(columns={'Silt _initial': 'Silt_initial'})\n",
        "\n",
        "# Texture_initialの数値への変換\n",
        "if 'Texture_initial' in rice_data.columns:\n",
        "    texture_mapping = {\n",
        "        # 細粒土壌（粘土質）- 粒子が最も小さい\n",
        "        'Clay': 1,             # 粘土\n",
        "        'Silty clay': 1,       # シルト質粘土\n",
        "        'Sandy clay': 1,       # 砂質粘土\n",
        "\n",
        "        # 中粒土壌（壌土質）- 中間の粒子サイズ\n",
        "        'Silty clay loam': 2,  # シルト質粘土質壌土\n",
        "        'Clay loam': 2,        # 粘土質壌土\n",
        "        'Sandy clay loam': 2,  # 砂質粘土質壌土\n",
        "        'Silty loam': 2,       # シルト質壌土\n",
        "        'Loam': 2,             # 壌土\n",
        "        'Loamy clay': 2,       # 壌土質粘土\n",
        "\n",
        "        # 粗粒土壌（砂質）- 粒子が最も大きい\n",
        "        'Sandy loam': 3,       # 砂質壌土\n",
        "        'Loamy sand': 3,       # 壌土質砂\n",
        "        'Sand': 3              # 砂\n",
        "    }\n",
        "\n",
        "    # 前処理：未知の値をNaNとして扱う\n",
        "    def map_texture(texture):\n",
        "        if pd.isna(texture) or texture == '' or not isinstance(texture, str):\n",
        "            return np.nan\n",
        "        texture = texture.strip()\n",
        "        return texture_mapping.get(texture, np.nan)\n",
        "\n",
        "    rice_data['Texture_initial'] = rice_data['Texture_initial'].apply(map_texture)\n",
        "\n",
        "# PyrolysisTemperature_biocharの数値への変換\n",
        "if 'PyrolysisTemperature_biochar' in rice_data.columns:\n",
        "    def fixed_convert_pyrolysis_temp(temp_val):\n",
        "        # NaNの場合\n",
        "        if pd.isna(temp_val):\n",
        "            return np.nan\n",
        "\n",
        "        # すでに数値型の場合はそのまま返す\n",
        "        if isinstance(temp_val, (int, float)):\n",
        "            return float(temp_val)\n",
        "\n",
        "        # 文字列型の処理\n",
        "        if isinstance(temp_val, str):\n",
        "            temp_str = temp_val.strip()\n",
        "\n",
        "            if temp_str == '':\n",
        "                return np.nan\n",
        "\n",
        "            # 範囲の処理\n",
        "            if '-' in temp_str:\n",
        "                try:\n",
        "                    parts = temp_str.split('-')\n",
        "                    low = float(parts[0].strip())\n",
        "                    high = float(parts[1].strip())\n",
        "                    return (low + high) / 2\n",
        "                except:\n",
        "                    return np.nan\n",
        "\n",
        "            # 単一値の処理\n",
        "            try:\n",
        "                return float(temp_str)\n",
        "            except:\n",
        "                return np.nan\n",
        "\n",
        "        return np.nan\n",
        "\n",
        "    rice_data['PyrolysisTemperature_biochar'] = rice_data['PyrolysisTemperature_biochar'].apply(fixed_convert_pyrolysis_temp)\n",
        "\n",
        "# Type_biocharの数値への変換\n",
        "if 'Type_biochar' in rice_data.columns:\n",
        "    # 論文を参考にした分類: W-R (Wood residues), ST-R (straw residues),\n",
        "    # L-M (livestock manure), and SH-R (shell residues)\n",
        "    biochar_mapping = {\n",
        "        # 1: 木質系バイオマス (Wood residues)\n",
        "        'Wood': 1,\n",
        "        'Eucalyptus wood': 1,\n",
        "        'Pine chip': 1,\n",
        "        'Pine': 1,\n",
        "        'Walnut shell': 1,\n",
        "        'Wood sawdust': 1,\n",
        "        'Spruce and Beech': 1,\n",
        "        'Jarrah': 1,\n",
        "        'Trunks and branches': 1,\n",
        "        'Acacia tree': 1,\n",
        "        'Poplar and plane tree wood': 1,\n",
        "        'Subabul stem and twigs': 1,\n",
        "        'Hardwood woodchips': 1,\n",
        "        'Hardwood woodchips(primarily oak,elm and hickory)': 1,\n",
        "        'Gliricidia sepium': 1,\n",
        "        'Eupatorium plant': 1,\n",
        "        'Eupatorium adenophorum': 1,\n",
        "        'Acacia mangium bark': 1,\n",
        "        'Elaeagnus angustifolia residue': 1,\n",
        "        'Green Waste': 1,\n",
        "        'Olive tree prunings': 1,\n",
        "        'Apple branches': 1,\n",
        "\n",
        "        # 2: わら系残渣 (Straw residues)\n",
        "        'Wheat straw': 2,\n",
        "        'Rice straw': 2,\n",
        "        'Rice Straw': 2,\n",
        "        'Maize straw': 2,\n",
        "        'Rapeseed straw': 2,\n",
        "        'Rape straw': 2,\n",
        "        'peanut straw': 2,\n",
        "        'Peanut straw': 2,\n",
        "        'Soybean straw': 2,\n",
        "        'Barley straw': 2,\n",
        "        'Cotton straw': 2,\n",
        "        'Cotton stock': 2,\n",
        "        'Tobacco straw': 2,\n",
        "        'Vicia faba straw': 2,\n",
        "        'Faba bean straw': 2,\n",
        "        'Urad straw': 2,\n",
        "        'Mungbean straw': 2,\n",
        "        'Pea straw': 2,\n",
        "        'Straw': 2,\n",
        "        'Wheat and rice straw': 2,\n",
        "        'Bagasse': 2,\n",
        "        'Reed': 2,\n",
        "        ' Reed': 2,\n",
        "        'Miscanthus': 2,\n",
        "        'Vine': 2,\n",
        "        'Ugarcane straw': 2,\n",
        "        'Canola straw': 2,\n",
        "        'Potato straw': 2,\n",
        "        'Capsicum straw': 2,\n",
        "        'Maize silage': 2,\n",
        "        'Rice chaff': 2,\n",
        "        'Bamboo': 2,\n",
        "        'Bamboo ': 2,\n",
        "        'Cassava stem': 2,\n",
        "        'cyperus esculentus straw': 2,\n",
        "        'Crop straw': 2,\n",
        "        'Vegetable waste': 2,\n",
        "        'Sorghum biomass': 2,\n",
        "\n",
        "        # 3: 畜産廃棄物 (Livestock manure)\n",
        "        'Cow manure': 3,\n",
        "        'Cattle feedlot': 3,\n",
        "        'Chicken manure': 3,\n",
        "        'Poultry litter': 3,\n",
        "        'Poultry manure': 3,\n",
        "        'Pig manure': 3,\n",
        "        'Diseased pig': 3,\n",
        "        'Quail litter': 3,\n",
        "        'Manure': 3,\n",
        "        'Sludge': 3,\n",
        "        'sludge': 3,\n",
        "        'Sewage sludge': 3,\n",
        "        'Green manure compost': 3,\n",
        "        'Domestic waste': 3,\n",
        "        'Municipal biowaste': 3,\n",
        "        'Mushroom': 3,\n",
        "        'Mixed': 3,\n",
        "        'Charcoal': 3,\n",
        "\n",
        "        # 4: 殻系残渣 (Shell residues)\n",
        "        'Rice husk': 4,\n",
        "        'Peanut hull': 4,\n",
        "        'peanut hull': 4,\n",
        "        'Coconut shell': 4,\n",
        "        'Coconut husk': 4,\n",
        "        'Palm kernel shell': 4,\n",
        "        'Maize cob': 4,\n",
        "        'Maize cob ': 4,\n",
        "        'Maize cob-residues': 4,\n",
        "        'Oat husk': 4,\n",
        "        'Oat hull': 4,\n",
        "        'wheat bran': 4,\n",
        "        'Nut shell': 4,\n",
        "        'Cottonseed husk': 4,\n",
        "        'Cottonseed husk ': 4,\n",
        "        'Cacao shell': 4,\n",
        "        'Grape pomace': 4,\n",
        "        'Mixture of rice husk and shell of cotton seed': 4,\n",
        "        'mixture of rice husk and shell of cotton seed': 4\n",
        "    }\n",
        "\n",
        "    # 前処理：未知の値をNaNとして扱う\n",
        "    def map_biochar_type(biochar_type):\n",
        "        if pd.isna(biochar_type) or biochar_type == '' or not isinstance(biochar_type, str):\n",
        "            return np.nan\n",
        "        biochar_type = biochar_type.strip()\n",
        "        return biochar_mapping.get(biochar_type, np.nan)\n",
        "\n",
        "    # 元のカラムをコピーして保持\n",
        "    rice_data['Type_biochar_original'] = rice_data['Type_biochar']\n",
        "\n",
        "    # Type_biocharを数値に変換\n",
        "    rice_data['Type_biochar'] = rice_data['Type_biochar'].apply(map_biochar_type)\n",
        "\n",
        "    # 変換結果のサマリーを表示\n",
        "    print(\"\\n=== バイオ炭タイプの変換結果 ===\")\n",
        "    print(f\"1. 木質系バイオマス (Wood residues): {(rice_data['Type_biochar'] == 1).sum()}件\")\n",
        "    print(f\"2. わら系残渣 (Straw residues): {(rice_data['Type_biochar'] == 2).sum()}件\")\n",
        "    print(f\"3. 畜産廃棄物 (Livestock manure): {(rice_data['Type_biochar'] == 3).sum()}件\")\n",
        "    print(f\"4. 殻系残渣 (Shell residues): {(rice_data['Type_biochar'] == 4).sum()}件\")\n",
        "    print(f\"未分類/欠損値: {rice_data['Type_biochar'].isna().sum()}件\")\n",
        "\n",
        "# 説明変数と目的変数の準備\n",
        "X = rice_data[features]\n",
        "y = rice_data['Yield_Difference']\n",
        "\n",
        "# 各特徴量の欠損値の割合を計算\n",
        "missing_percentage = X.isnull().mean() * 100\n",
        "print(\"Missing data percentage for each feature (%):\")\n",
        "for feature, percentage in missing_percentage.items():\n",
        "    print(f\"{feature}: {percentage:.2f}%\")\n",
        "\n",
        "# 欠損値が50%以上の特徴量を除外\n",
        "high_missing_features = missing_percentage[missing_percentage >= 50].index.tolist()\n",
        "if high_missing_features:\n",
        "    print(f\"\\nExcluding features with missing data ≥ 50%: {high_missing_features}\")\n",
        "    X = X.drop(columns=high_missing_features)\n",
        "\n",
        "# 残りの説明変数の欠損値の数を確認\n",
        "print(f\"\\nNumber of missing values in explanatory variables after processing: {X.isnull().sum().sum()}\")\n",
        "\n",
        "# インデックスを明示的に保存\n",
        "original_index = X.index.copy()\n",
        "\n",
        "# KNN補間 (k=5)\n",
        "imputer = KNNImputer(n_neighbors=5)\n",
        "X_imputed = imputer.fit_transform(X)\n",
        "\n",
        "# データフレームに戻す - 重要: 元のインデックスを保持する\n",
        "X_processed = pd.DataFrame(X_imputed, columns=X.columns, index=original_index)\n",
        "\n",
        "# スケーリングを行わないように変更\n",
        "# X_processed をそのまま使用する\n",
        "\n",
        "# X_original と X_processed は同じインデックスを持っているので、loc の問題は解決されるはず\n",
        "X_original = rice_data[features].copy()\n",
        "# 欠損値が多い特徴量を除外した後の列名に合わせる\n",
        "if high_missing_features:\n",
        "    X_original = X_original.drop(columns=high_missing_features)\n",
        "# X_original と X_processed は同じインデックスを持っているので、loc の問題は解決されるはず\n",
        "\n",
        "# 確認のためのコード\n",
        "print(f\"X_original index shape: {X_original.index.shape}\")\n",
        "print(f\"X_processed index shape: {X_processed.index.shape}\")\n",
        "print(f\"共通するインデックスの数: {len(set(X_original.index) & set(X_processed.index))}\")\n",
        "\n",
        "# すべての説明変数および目的変数のヒストグラムを作成\n",
        "print(\"\\n補間後の説明変数と目的変数のヒストグラムを作成中...\")\n",
        "\n",
        "# 説明変数のヒストグラム\n",
        "for feature in X_processed.columns:\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.hist(X_processed[feature], bins=20, color='skyblue', edgecolor='black')\n",
        "    plt.title(f'Histogram of {feature} (After Imputation)')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{histograms_dir}/hist_{feature}.png')\n",
        "    plt.close()\n",
        "\n",
        "# 目的変数のヒストグラム\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.hist(y, bins=20, color='lightgreen', edgecolor='black')\n",
        "plt.title('Histogram of Yield_Difference (Target Variable)')\n",
        "plt.xlabel('Yield_Difference')\n",
        "plt.ylabel('Frequency')\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.tight_layout()\n",
        "plt.savefig(f'{histograms_dir}/hist_Yield_Difference.png')\n",
        "plt.close()\n",
        "\n",
        "# 補間前の元データの説明変数のヒストグラム (参考用)\n",
        "for feature in X.columns:\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.hist(X[feature].dropna(), bins=20, color='salmon', edgecolor='black')\n",
        "    plt.title(f'Histogram of {feature} (Before Imputation)')\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Frequency')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'{histograms_dir}/hist_{feature}_before_imputation.png')\n",
        "    plt.close()\n",
        "\n",
        "print(f\"ヒストグラムを {histograms_dir} フォルダに保存しました。\")\n",
        "\n",
        "# 国別のデータ\n",
        "countries = rice_data['Country'].unique()\n",
        "print(f\"データに含まれる国のリスト: {countries}\")\n",
        "\n",
        "country_instances = {}\n",
        "for country in countries:\n",
        "    country_instances[country] = rice_data[rice_data['Country'] == country].index\n",
        "    print(f\"{country}のデータ数: {len(country_instances[country])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#機械学習モデルの構築"
      ],
      "metadata": {
        "id": "5OTVWosgvpy9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "機械学習モデルの精度の比較"
      ],
      "metadata": {
        "id": "5s0Y1G3dvs-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============= ２．機械学習モデルの精度の比較 =============\n",
        "\n",
        "# 必要なインポートを追加\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import r2_score, mean_squared_error\n",
        "from sklearn.cross_decomposition import PLSRegression\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split\n",
        "from xgboost import XGBRegressor\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization, Activation\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "import os\n",
        "from itertools import product\n",
        "\n",
        "# 警告を無視\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# モデルの評価関数\n",
        "def evaluate_model(y_true, y_pred):\n",
        "    r2 = r2_score(y_true, y_pred)\n",
        "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "    rrmse = rmse / np.mean(np.abs(y_true)) * 100 if np.mean(np.abs(y_true)) != 0 else np.nan\n",
        "    return r2, rmse, rrmse\n",
        "\n",
        "# 交差検証の実行関数\n",
        "def run_cross_validation(model, X, y, kf):\n",
        "    \"\"\"\n",
        "    与えられたモデルで交差検証を実行し、予測値を返す\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    model : モデルインスタンス\n",
        "    X : 特徴量データ\n",
        "    y : 目的変数\n",
        "    kf : KFold オブジェクト\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    y_pred_cv : 交差検証の予測値\n",
        "    \"\"\"\n",
        "    y_pred_cv = np.zeros_like(y)\n",
        "\n",
        "    for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
        "        X_train, X_test = X[train_idx], X[test_idx]\n",
        "        y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "        # モデルを訓練\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # テストデータで予測\n",
        "        y_pred_cv[test_idx] = model.predict(X_test)\n",
        "\n",
        "    return y_pred_cv\n",
        "\n",
        "# ニューラルネットワークのクロスバリデーション\n",
        "def run_nn_cross_validation(X, y, kf, params):\n",
        "    \"\"\"\n",
        "    ニューラルネットワークの交差検証を実行し、予測値を返す\n",
        "\n",
        "    Parameters:\n",
        "    -----------\n",
        "    X : 特徴量データ\n",
        "    y : 目的変数\n",
        "    kf : KFold オブジェクト\n",
        "    params : モデルパラメータ辞書\n",
        "\n",
        "    Returns:\n",
        "    --------\n",
        "    y_pred_cv : 交差検証の予測値\n",
        "    \"\"\"\n",
        "    y_pred_cv = np.zeros_like(y)\n",
        "\n",
        "    for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
        "        print(f\"  フォールド {fold+1}/5 処理中...\")\n",
        "        X_train, X_test = X[train_idx], X[test_idx]\n",
        "        y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "        # 検証データをさらに分割\n",
        "        X_train_split, X_val, y_train_split, y_val = train_test_split(\n",
        "            X_train, y_train, test_size=0.2, random_state=42\n",
        "        )\n",
        "\n",
        "        # モデルを定義 (固定の3層構造)\n",
        "        model = Sequential()\n",
        "\n",
        "        # 入力層\n",
        "        model.add(Dense(params['units'], input_shape=(X.shape[1],)))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(Dropout(params['dropout']))\n",
        "\n",
        "        # 隠れ層1\n",
        "        model.add(Dense(params['units']))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Activation('relu'))\n",
        "        #model.add(Dropout(params['dropout']))\n",
        "\n",
        "        # 隠れ層2\n",
        "        model.add(Dense(params['units']))\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Activation('relu'))\n",
        "        #model.add(Dropout(params['dropout']))\n",
        "\n",
        "        # 出力層\n",
        "        model.add(Dense(1))\n",
        "\n",
        "        # コンパイル\n",
        "        optimizer = keras.optimizers.Adam(learning_rate=params['learning_rate'])\n",
        "        model.compile(loss='mse', optimizer=optimizer, metrics=['mae'])\n",
        "\n",
        "        # Early Stoppingを設定\n",
        "        early_stopping = EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=10,\n",
        "            restore_best_weights=True,\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # モデルを訓練\n",
        "        model.fit(\n",
        "            X_train_split, y_train_split,\n",
        "            epochs=100,\n",
        "            batch_size=params['batch_size'],\n",
        "            validation_data=(X_val, y_val),\n",
        "            callbacks=[early_stopping],\n",
        "            verbose=0\n",
        "        )\n",
        "\n",
        "        # テストデータで予測\n",
        "        y_pred_cv[test_idx] = model.predict(X_test, verbose=0).flatten()\n",
        "\n",
        "        # メモリ解放\n",
        "        tf.keras.backend.clear_session()\n",
        "\n",
        "    return y_pred_cv\n",
        "\n",
        "# ======================= 各アルゴリズムのハイパーパラメータ定義 =======================\n",
        "\n",
        "# モデルとハイパーパラメータグリッドの定義\n",
        "model_params = {\n",
        "    'PLSR': {\n",
        "        'model': PLSRegression,\n",
        "        'params': {\n",
        "            'n_components': [1, 3, 5, 10, 15]\n",
        "        }\n",
        "    },\n",
        "    'Decision Tree': {\n",
        "        'model': DecisionTreeRegressor,\n",
        "        'params': {\n",
        "            'max_depth': [5, 10, 15, None],\n",
        "            'min_samples_split': [2, 5, 10]\n",
        "        }\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'model': RandomForestRegressor,\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'max_depth': [10, 20, None]\n",
        "        }\n",
        "    },\n",
        "    'XGBoost': {\n",
        "        'model': XGBRegressor,\n",
        "        'params': {\n",
        "            'n_estimators': [50, 100, 200],\n",
        "            'max_depth': [3, 5, 7],\n",
        "            'learning_rate': [0.01, 0.1, 0.2]\n",
        "        }\n",
        "    },\n",
        "    'Neural Network': {\n",
        "        'params': {\n",
        "            'layers': [3],        # 3層で固定\n",
        "            'units': [32],        # 32ユニットで固定\n",
        "            'dropout': [0.2],     # ドロップアウト率0.2で固定\n",
        "            'learning_rate': [0.001, 0.01, 0.1],  # 学習率のみ探索\n",
        "            'batch_size': [8, 16, 32]              # バッチサイズのみ探索\n",
        "        }\n",
        "    }\n",
        "}\n",
        "\n",
        "# 結果を格納するためのデータフレーム\n",
        "results = pd.DataFrame(columns=['Algorithm', 'Best_Params', 'CV_R2', 'CV_RMSE', 'CV_RRMSE'])\n",
        "\n",
        "# 5分割交差検証\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# データをnumpy配列に変換\n",
        "X_data = X_processed.values if isinstance(X_processed, pd.DataFrame) else X_processed\n",
        "y_data = y.values if isinstance(y, pd.Series) else y\n",
        "\n",
        "print(\"\\n======= 5分割交差検証によるモデル評価（シンプル版） =======\")\n",
        "\n",
        "# 各アルゴリズムの評価\n",
        "for name, config in model_params.items():\n",
        "    print(f\"\\n{name}の評価中...\")\n",
        "\n",
        "    # ニューラルネットワークの場合\n",
        "    if name == 'Neural Network':\n",
        "        best_rmse = float('inf')\n",
        "        best_params = None\n",
        "        best_r2 = -float('inf')\n",
        "        best_rrmse = float('inf')\n",
        "        best_y_pred = None\n",
        "\n",
        "        # ハイパーパラメータの組み合わせごとに評価\n",
        "        param_list = list(product(\n",
        "            config['params']['layers'],\n",
        "            config['params']['units'],\n",
        "            config['params']['dropout'],\n",
        "            config['params']['learning_rate'],\n",
        "            config['params']['batch_size']\n",
        "        ))\n",
        "\n",
        "        for layers, units, dropout, lr, batch_size in param_list:\n",
        "            print(f\"  パラメータ評価中: layers={layers}, units={units}, dropout={dropout}, lr={lr}, batch_size={batch_size}\")\n",
        "\n",
        "            params = {\n",
        "                'layers': layers,\n",
        "                'units': units,\n",
        "                'dropout': dropout,\n",
        "                'learning_rate': lr,\n",
        "                'batch_size': batch_size\n",
        "            }\n",
        "\n",
        "            # 交差検証を実行\n",
        "            y_pred_cv = run_nn_cross_validation(X_data, y_data, kf, params)\n",
        "\n",
        "            # 評価指標を計算\n",
        "            r2, rmse, rrmse = evaluate_model(y_data, y_pred_cv)\n",
        "\n",
        "            print(f\"    RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
        "\n",
        "            # 最良のモデルを更新\n",
        "            if rmse < best_rmse:\n",
        "                best_rmse = rmse\n",
        "                best_r2 = r2\n",
        "                best_rrmse = rrmse\n",
        "                best_params = params\n",
        "                best_y_pred = y_pred_cv\n",
        "\n",
        "        # 最良のパラメータと結果を表示\n",
        "        print(f\"\\n最良のパラメータ: {best_params}\")\n",
        "        print(f\"CV R²: {best_r2:.4f}, CV RMSE: {best_rmse:.4f}, CV RRMSE: {best_rrmse:.2f}%\")\n",
        "\n",
        "    # その他のアルゴリズムの場合\n",
        "    else:\n",
        "        best_rmse = float('inf')\n",
        "        best_params = None\n",
        "        best_r2 = -float('inf')\n",
        "        best_rrmse = float('inf')\n",
        "        best_y_pred = None\n",
        "\n",
        "        # パラメータの組み合わせを生成\n",
        "        param_keys = list(config['params'].keys())\n",
        "        param_values = list(config['params'].values())\n",
        "        param_combinations = list(product(*param_values))\n",
        "\n",
        "        # 各パラメータの組み合わせを評価\n",
        "        for params_tuple in param_combinations:\n",
        "            params_dict = {k: v for k, v in zip(param_keys, params_tuple)}\n",
        "            param_str = \", \".join([f\"{k}={v}\" for k, v in params_dict.items()])\n",
        "            print(f\"  パラメータ評価中: {param_str}\")\n",
        "\n",
        "            # モデルを初期化\n",
        "            if name == 'PLSR':\n",
        "                # PLSRは初期化方法が異なるので特別扱い\n",
        "                model = config['model'](**params_dict)\n",
        "            else:\n",
        "                model = config['model'](random_state=42, **params_dict)\n",
        "\n",
        "            # 交差検証を実行\n",
        "            y_pred_cv = run_cross_validation(model, X_data, y_data, kf)\n",
        "\n",
        "            # 評価指標を計算\n",
        "            r2, rmse, rrmse = evaluate_model(y_data, y_pred_cv)\n",
        "\n",
        "            print(f\"    RMSE: {rmse:.4f}, R²: {r2:.4f}\")\n",
        "\n",
        "            # 最良のモデルを更新\n",
        "            if rmse < best_rmse:\n",
        "                best_rmse = rmse\n",
        "                best_r2 = r2\n",
        "                best_rrmse = rrmse\n",
        "                best_params = params_dict\n",
        "                best_y_pred = y_pred_cv\n",
        "\n",
        "        # 最良のパラメータと結果を表示\n",
        "        print(f\"\\n最良のパラメータ: {best_params}\")\n",
        "        print(f\"CV R²: {best_r2:.4f}, CV RMSE: {best_rmse:.4f}, CV RRMSE: {best_rrmse:.2f}%\")\n",
        "\n",
        "    # 交差検証による予測と実測値の散布図\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.scatter(y_data, best_y_pred, alpha=0.6)\n",
        "\n",
        "    min_val = min(min(y_data), min(best_y_pred))\n",
        "    max_val = max(max(y_data), max(best_y_pred))\n",
        "\n",
        "    # 1:1ラインを追加\n",
        "    plt.plot([min_val, max_val], [min_val, max_val], 'r--')\n",
        "\n",
        "    plt.xlabel('Observed')\n",
        "    plt.ylabel('Cross-Validation Predicted')\n",
        "    plt.title(f'{name}: Cross-Validation Predictions vs Observed\\nR² = {best_r2:.4f}, RMSE = {best_rmse:.4f}')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.savefig(os.path.join('section2_model_comparison', f'{name}_cv_scatter_simple.png'))\n",
        "    plt.close()\n",
        "\n",
        "    # 結果をデータフレームに追加\n",
        "    results = pd.concat([results, pd.DataFrame({\n",
        "        'Algorithm': [name],\n",
        "        'Best_Params': [str(best_params)],\n",
        "        'CV_R2': [best_r2],\n",
        "        'CV_RMSE': [best_rmse],\n",
        "        'CV_RRMSE': [best_rrmse]\n",
        "    })], ignore_index=True)\n",
        "\n",
        "# 結果を保存\n",
        "results = results.sort_values('CV_R2', ascending=False)\n",
        "results.to_excel(os.path.join('section2_model_comparison', 'cross_validation_results_simple.xlsx'), index=False)\n",
        "print(\"\\n交差検証による比較結果:\")\n",
        "print(results)\n",
        "\n",
        "# 決定係数の棒グラフ作成\n",
        "plt.figure(figsize=(10, 6))\n",
        "ax = sns.barplot(x='Algorithm', y='CV_R2', data=results, palette='viridis')\n",
        "plt.title('5-fold cross validation')\n",
        "plt.ylabel('R²')\n",
        "plt.xlabel('Algorithm')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# 各バーの上に値を表示\n",
        "for i, v in enumerate(results['CV_R2']):\n",
        "    ax.text(i, v + 0.01, f'{v:.4f}', ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join('section2_model_comparison', 'algorithm_comparison_cv_r2_simple.png'))\n",
        "plt.close()\n",
        "\n",
        "# RMSEの棒グラフ作成\n",
        "plt.figure(figsize=(10, 6))\n",
        "ax = sns.barplot(x='Algorithm', y='CV_RMSE', data=results, palette='coolwarm_r')\n",
        "plt.title('5-fold cross validation')\n",
        "plt.ylabel('RMSE')\n",
        "plt.xlabel('Algorithm')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# 各バーの上に値を表示\n",
        "for i, v in enumerate(results['CV_RMSE']):\n",
        "    ax.text(i, v + 0.01, f'{v:.4f}', ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join('section2_model_comparison', 'algorithm_comparison_cv_rmse_simple.png'))\n",
        "plt.close()\n",
        "\n",
        "# RRMSEの棒グラフ作成\n",
        "plt.figure(figsize=(10, 6))\n",
        "ax = sns.barplot(x='Algorithm', y='CV_RRMSE', data=results, palette='coolwarm_r')\n",
        "plt.title('5-fold cross validation')\n",
        "plt.ylabel('RRMSE (%)')\n",
        "plt.xlabel('Algorithm')\n",
        "plt.xticks(rotation=45)\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "\n",
        "# 各バーの上に値を表示\n",
        "for i, v in enumerate(results['CV_RRMSE']):\n",
        "    ax.text(i, v + 0.5, f'{v:.2f}%', ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join('section2_model_comparison', 'algorithm_comparison_cv_rrmse_simple.png'))\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "8p4Da5KhuNoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SHAP関連解析用の機械学習モデルを全データにより構築"
      ],
      "metadata": {
        "id": "QEbrJqJmv3sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============= ３．最終モデルの構築 =============\n",
        "\n",
        "# 2で保存した最適なランダムフォレストのパラメータを使用\n",
        "print(\"\\nランダムフォレストの最終モデル構築中...\")\n",
        "\n",
        "# 交差検証結果から最適なパラメータを読み込む\n",
        "try:\n",
        "    cv_results = pd.read_excel(os.path.join('section2_model_comparison', 'cross_validation_results_simple.xlsx'))\n",
        "\n",
        "    # Random Forestモデルの結果を取得\n",
        "    rf_results = cv_results[cv_results['Algorithm'] == 'Random Forest']\n",
        "\n",
        "    if not rf_results.empty:\n",
        "        # ベストパラメータの文字列を取得してdictに変換\n",
        "        best_params_str = rf_results.iloc[0]['Best_Params']\n",
        "\n",
        "        # 文字列形式のパラメータを辞書に変換\n",
        "        # 例: \"{'n_estimators': 100, 'max_depth': 20}\" → {'n_estimators': 100, 'max_depth': 20}\n",
        "        import ast\n",
        "        best_rf_params = ast.literal_eval(best_params_str)\n",
        "\n",
        "        print(f\"セクション2から取得した最適パラメータ: {best_rf_params}\")\n",
        "    else:\n",
        "        # Random Forestの結果が見つからない場合はデフォルトパラメータを使用\n",
        "        print(\"警告: Random Forestの結果が見つかりません。デフォルトパラメータを使用します。\")\n",
        "        best_rf_params = {'n_estimators': 100, 'max_depth': None}\n",
        "except Exception as e:\n",
        "    print(f\"警告: 交差検証結果の読み込みでエラーが発生しました: {e}\")\n",
        "    print(\"デフォルトパラメータを使用します。\")\n",
        "    best_rf_params = {'n_estimators': 100, 'max_depth': None}\n",
        "\n",
        "# 最終モデルの構築\n",
        "final_rf_model = RandomForestRegressor(random_state=42, **best_rf_params)\n",
        "final_rf_model.fit(X_processed, y)\n",
        "\n",
        "# 訓練データでの予測\n",
        "y_train_pred = final_rf_model.predict(X_processed)\n",
        "r2_train, rmse_train, rrmse_train = evaluate_model(y, y_train_pred)\n",
        "\n",
        "print(f\"訓練データでの決定係数 (R²): {r2_train:.4f}\")\n",
        "print(f\"訓練データでのRMSE: {rmse_train:.4f}\")\n",
        "print(f\"訓練データでのRRMSE: {rrmse_train:.4f}%\")\n",
        "\n",
        "# 訓練データの散布図\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.scatter(y, y_train_pred)\n",
        "min_val = min(min(y), min(y_train_pred))\n",
        "max_val = max(max(y), max(y_train_pred))\n",
        "\n",
        "# 正方形のプロットエリアを確保するために軸の範囲を調整\n",
        "overall_min = min(min_val, min_val)\n",
        "overall_max = max(max_val, max_val)\n",
        "plt.xlim(overall_min, overall_max)\n",
        "plt.ylim(overall_min, overall_max)\n",
        "\n",
        "plt.plot([overall_min, overall_max], [overall_min, overall_max], 'r--')\n",
        "plt.xlabel('Observed')\n",
        "plt.ylabel('Predicted')\n",
        "plt.title(f'Final Random Forest Model: Predicted vs Observed (R² = {r2_train:.4f}, RMSE = {rmse_train:.4f}, RRMSE = {rrmse_train:.2f}%)')\n",
        "plt.grid(True)\n",
        "plt.savefig(os.path.join('section3_final_model', 'RandomForest_Final_scatter.png'))\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "DFM9zdh4uRJW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#SHAP関連解析"
      ],
      "metadata": {
        "id": "sXPjxaRqvci1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "予測値の上位3サンプルと下位3サンプル（国の重複を避ける）のSHAP値の可視化"
      ],
      "metadata": {
        "id": "GQX1eVt5vP0s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============= ４．すべてのインスタンスの収量差予測の表示と極値の分析 =============\n",
        "\n",
        "# SHAP値による解釈\n",
        "print(\"\\nSHAP値による解釈を開始...\")\n",
        "\n",
        "# SHAP値の計算\n",
        "explainer = shap.TreeExplainer(final_rf_model)\n",
        "shap_values = explainer.shap_values(X_processed)\n",
        "\n",
        "# ======= 4.1 すべてのインスタンスの予測値を世界地図上にプロット =======\n",
        "\n",
        "print(\"\\n1. すべてのインスタンスの予測値を世界地図上にプロット中...\")\n",
        "\n",
        "# 必要なライブラリのインポート\n",
        "try:\n",
        "    import matplotlib.pyplot as plt\n",
        "    import cartopy.crs as ccrs\n",
        "    import cartopy.feature as cfeature\n",
        "    print(\"必要なライブラリは既にインストールされています。\")\n",
        "except ImportError:\n",
        "    print(\"必要なライブラリをインストールしています...\")\n",
        "    !pip install matplotlib cartopy\n",
        "    import matplotlib.pyplot as plt\n",
        "    import cartopy.crs as ccrs\n",
        "    import cartopy.feature as cfeature\n",
        "\n",
        "# 各インスタンスの位置情報と予測値を抽出\n",
        "all_instances_data = []\n",
        "for idx in X_processed.index:\n",
        "    if idx in rice_data.index:\n",
        "        instance = rice_data.loc[idx]\n",
        "        country = instance['Country'] if 'Country' in instance else 'Unknown'\n",
        "\n",
        "        # 緯度と経度の情報があるか確認\n",
        "        if 'Latitude' in instance and 'Longitude' in instance:\n",
        "            lat = instance['Latitude']\n",
        "            lon = instance['Longitude']\n",
        "\n",
        "            if pd.notna(lat) and pd.notna(lon):\n",
        "                # 予測値と実測値を取得\n",
        "                pred_value = final_rf_model.predict(X_processed.loc[idx:idx])[0]\n",
        "                actual_value = y.loc[idx]\n",
        "\n",
        "                all_instances_data.append({\n",
        "                    'Index': idx,\n",
        "                    'Country': country,\n",
        "                    'Latitude': lat,\n",
        "                    'Longitude': lon,\n",
        "                    'PredictedValue': pred_value,\n",
        "                    'ActualValue': actual_value,\n",
        "                    'AbsPredicted': abs(pred_value)\n",
        "                })\n",
        "\n",
        "# データが見つからない場合はデフォルトの国の位置情報を使用\n",
        "if len(all_instances_data) < len(X_processed) * 0.5:  # データの半分以上が位置情報を持っていない場合\n",
        "    print(\"\\n多くのインスタンスに位置情報がありません。国のデフォルト位置を使用します...\")\n",
        "\n",
        "    # 国のデフォルト位置\n",
        "    country_locations = {\n",
        "        'China': {'lat': 35.86, 'lon': 104.19},\n",
        "        'India': {'lat': 20.59, 'lon': 78.96},\n",
        "        'Japan': {'lat': 36.20, 'lon': 138.25},\n",
        "        'Indonesia': {'lat': -0.79, 'lon': 113.92},\n",
        "        'Brazil': {'lat': -14.24, 'lon': -51.93},\n",
        "        'United States': {'lat': 37.09, 'lon': -95.71},\n",
        "        'Australia': {'lat': -25.27, 'lon': 133.77},\n",
        "        'Italy': {'lat': 41.87, 'lon': 12.57},\n",
        "        'Spain': {'lat': 40.46, 'lon': -3.75},\n",
        "        'France': {'lat': 46.23, 'lon': 2.21},\n",
        "        'Germany': {'lat': 51.17, 'lon': 10.45},\n",
        "        'United Kingdom': {'lat': 55.38, 'lon': -3.44},\n",
        "        'Canada': {'lat': 56.13, 'lon': -106.35},\n",
        "        'Russia': {'lat': 61.52, 'lon': 105.32},\n",
        "        'South Korea': {'lat': 35.91, 'lon': 127.77},\n",
        "        'Philippines': {'lat': 12.88, 'lon': 121.77},\n",
        "        'Vietnam': {'lat': 14.06, 'lon': 108.28},\n",
        "        'Thailand': {'lat': 15.87, 'lon': 100.99},\n",
        "        'Malaysia': {'lat': 4.21, 'lon': 101.98},\n",
        "        'Bangladesh': {'lat': 23.68, 'lon': 90.35},\n",
        "        'Myanmar': {'lat': 21.92, 'lon': 95.96},\n",
        "        'Pakistan': {'lat': 30.38, 'lon': 69.35},\n",
        "        'Sri Lanka': {'lat': 7.87, 'lon': 80.77},\n",
        "        'Nepal': {'lat': 28.39, 'lon': 84.12},\n",
        "        'Nigeria': {'lat': 9.08, 'lon': 8.67},\n",
        "        'Egypt': {'lat': 26.82, 'lon': 30.80},\n",
        "        'Turkey': {'lat': 38.96, 'lon': 35.24},\n",
        "        'Iran': {'lat': 32.43, 'lon': 53.69},\n",
        "        'Mexico': {'lat': 23.63, 'lon': -102.55},\n",
        "        'Argentina': {'lat': -38.42, 'lon': -63.62},\n",
        "        'Colombia': {'lat': 4.57, 'lon': -74.30},\n",
        "        'Peru': {'lat': -9.19, 'lon': -75.00},\n",
        "        'Chile': {'lat': -35.68, 'lon': -71.54}\n",
        "    }\n",
        "\n",
        "    # 国ごとにインスタンスをグループ化\n",
        "    country_instances = {}\n",
        "    for idx in X_processed.index:\n",
        "        country = rice_data.loc[idx, 'Country'] if 'Country' in rice_data.columns and idx in rice_data.index else 'Unknown'\n",
        "\n",
        "        if country not in country_instances:\n",
        "            country_instances[country] = []\n",
        "        country_instances[country].append(idx)\n",
        "\n",
        "    # 各国のインスタンスに対して位置情報を追加\n",
        "    all_instances_data = []\n",
        "    for country, instances in country_instances.items():\n",
        "        if country in country_locations:\n",
        "            lat = country_locations[country]['lat']\n",
        "            lon = country_locations[country]['lon']\n",
        "\n",
        "            # 同じ国の複数のインスタンスを少しオフセットして表示\n",
        "            import random\n",
        "            random.seed(42)  # 再現性のため\n",
        "\n",
        "            for idx in instances:\n",
        "                if idx in X_processed.index and idx in y.index:\n",
        "                    # 小さなランダムオフセットを適用\n",
        "                    offset_scale = min(3, max(1, len(instances) / 10))\n",
        "                    lat_offset = (random.random() - 0.5) * offset_scale\n",
        "                    lon_offset = (random.random() - 0.5) * offset_scale\n",
        "\n",
        "                    # 予測値と実測値を取得\n",
        "                    pred_value = final_rf_model.predict(X_processed.loc[idx:idx])[0]\n",
        "                    actual_value = y.loc[idx] if idx in y.index else np.nan\n",
        "\n",
        "                    all_instances_data.append({\n",
        "                        'Index': idx,\n",
        "                        'Country': country,\n",
        "                        'Latitude': lat + lat_offset,\n",
        "                        'Longitude': lon + lon_offset,\n",
        "                        'PredictedValue': pred_value,\n",
        "                        'ActualValue': actual_value,\n",
        "                        'AbsPredicted': abs(pred_value)\n",
        "                    })\n",
        "        else:\n",
        "            print(f\"警告: {country}のデフォルト位置情報がありません。\")\n",
        "\n",
        "# データフレームに変換\n",
        "if all_instances_data:\n",
        "    all_df = pd.DataFrame(all_instances_data)\n",
        "    print(f\"合計 {len(all_df)} インスタンスの位置情報と予測値を取得しました。\")\n",
        "\n",
        "    # 世界地図の作成\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    ax = plt.axes(projection=ccrs.PlateCarree())\n",
        "\n",
        "    # 地図の基本要素を追加\n",
        "    ax.add_feature(cfeature.LAND, facecolor='lightgray')\n",
        "    ax.add_feature(cfeature.OCEAN, facecolor='lightblue')\n",
        "    ax.add_feature(cfeature.COASTLINE, linewidth=0.5)\n",
        "    ax.add_feature(cfeature.BORDERS, linewidth=0.5)\n",
        "    ax.gridlines(draw_labels=True, linewidth=0.5, color='gray', alpha=0.5, linestyle='--')\n",
        "    ax.set_global()\n",
        "\n",
        "    # 各インスタンスをプロット\n",
        "    marker_size = 10  # 統一サイズ\n",
        "\n",
        "    # カラーマップの作成\n",
        "    import matplotlib as mpl\n",
        "    import numpy as np\n",
        "\n",
        "    # -0.5から+2.5の範囲で青から赤のグラデーションを定義\n",
        "    cmap = mpl.colors.LinearSegmentedColormap.from_list(\n",
        "        'custom_diverging',\n",
        "        [(0, 'darkblue'), (0.5, 'white'), (1, 'darkred')],\n",
        "        N=256\n",
        "    )\n",
        "\n",
        "    # 値の範囲を-0.5から2.5に設定\n",
        "    norm = mpl.colors.Normalize(vmin=-0.5, vmax=2.5)\n",
        "\n",
        "    # 各インスタンスをプロット（色分けを修正）\n",
        "    for _, row in all_df.iterrows():\n",
        "        # 予測値に基づいて色を決定\n",
        "        # normとcmapを使って、-0.5から2.5の範囲で青から赤への色を決定\n",
        "        color = cmap(norm(row['PredictedValue']))\n",
        "\n",
        "        # マーカープロット - 透明度は一定に\n",
        "        ax.scatter(row['Longitude'], row['Latitude'], s=marker_size, color=color,\n",
        "                  alpha=0.7, transform=ccrs.PlateCarree(), edgecolor='black', linewidth=0.3)\n",
        "\n",
        "    # カラーバーの設定\n",
        "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
        "    sm.set_array([])\n",
        "    cbar = plt.colorbar(sm, ax=ax, shrink=0.6, pad=0.02)\n",
        "    cbar.set_ticklabels(['-0.5', '0', '1', '2', '2.5'])\n",
        "\n",
        "    plt.title('All Rice Instances: Predicted Yield Difference\\n(Blue = Negative, Red = Positive)', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join('section4_predictions_analysis', 'All_Instances_Prediction_Map.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(\"すべてのインスタンスの予測マップを 'section4_predictions_analysis/All_Instances_Prediction_Map.png' に保存しました。\")\n",
        "\n",
        "    # インスタンスデータをCSVに保存\n",
        "    all_df.to_csv(os.path.join('section4_predictions_analysis', 'All_Instances_Predictions.csv'), index=False)\n",
        "    print(\"すべてのインスタンスの予測データを 'section4_predictions_analysis/All_Instances_Predictions.csv' に保存しました。\")\n",
        "else:\n",
        "    print(\"位置情報を持つインスタンスが見つかりませんでした。マップは生成されません。\")\n",
        "\n",
        "# ======= 4.2 極値（上位3件、下位3件）の選択 =======\n",
        "\n",
        "print(\"\\n2. 予測収量差の極値を選択中（上位3件、下位3件）...\")\n",
        "\n",
        "# データフレームが存在することを確認\n",
        "if 'all_df' in locals() and not all_df.empty:\n",
        "    # 予測値で降順と昇順にソート\n",
        "    sorted_high = all_df.sort_values('PredictedValue', ascending=False)\n",
        "    sorted_low = all_df.sort_values('PredictedValue', ascending=True)\n",
        "\n",
        "    # 上位グループ内で国の重複を避けながら上位3件を取得\n",
        "    top_3_countries = []\n",
        "    top_3_instances = []\n",
        "\n",
        "    for _, row in sorted_high.iterrows():\n",
        "        if row['Country'] not in top_3_countries:\n",
        "            top_3_countries.append(row['Country'])\n",
        "            top_3_instances.append(row)\n",
        "            if len(top_3_instances) >= 3:  # 3件に変更\n",
        "                break\n",
        "\n",
        "    # 下位グループ内で国の重複を避けながら下位3件を取得\n",
        "    bottom_3_countries = []\n",
        "    bottom_3_instances = []\n",
        "\n",
        "    for _, row in sorted_low.iterrows():\n",
        "        if row['Country'] not in bottom_3_countries:\n",
        "            bottom_3_countries.append(row['Country'])\n",
        "            bottom_3_instances.append(row)\n",
        "            if len(bottom_3_instances) >= 3:  # 3件に変更\n",
        "                break\n",
        "\n",
        "    # 6個のインスタンスを一つのデータフレームに結合\n",
        "    extremes_df = pd.DataFrame(top_3_instances + bottom_3_instances)\n",
        "\n",
        "    # 確認のため、選択したインスタンスを表示\n",
        "    print(\"\\n選択された6個の極値インスタンス:\")\n",
        "    print(\"上位3件（高い収量差）:\")\n",
        "    for i, row in enumerate(top_3_instances, 1):\n",
        "        print(f\"{i}. {row['Country']}: 予測値 = {row['PredictedValue']:.4f}, 実測値 = {row['ActualValue']:.4f}\")\n",
        "\n",
        "    print(\"\\n下位3件（低い収量差）:\")\n",
        "    for i, row in enumerate(bottom_3_instances, 1):\n",
        "        print(f\"{i}. {row['Country']}: 予測値 = {row['PredictedValue']:.4f}, 実測値 = {row['ActualValue']:.4f}\")\n",
        "\n",
        "# 極値インスタンスの地図を生成する部分を修正\n",
        "# 中国の複数インスタンスのラベルが重ならないように調整\n",
        "\n",
        "if 'extremes_df' in locals() and not extremes_df.empty:\n",
        "    # 重なりを解消するための調整\n",
        "    # まず極値データフレームのコピーを作成\n",
        "    extremes_df_adjusted = extremes_df.copy()\n",
        "\n",
        "    # 中国のインスタンスを特定\n",
        "    china_instances = extremes_df_adjusted[extremes_df_adjusted['Country'] == 'China']\n",
        "\n",
        "    # 中国のインスタンスが複数ある場合\n",
        "    if len(china_instances) > 1:\n",
        "        print(f\"中国の複数インスタンス ({len(china_instances)}件) のラベル位置を調整します。\")\n",
        "\n",
        "        # 中国のインスタンスそれぞれに異なるオフセットを適用\n",
        "        for i, idx in enumerate(china_instances.index):\n",
        "            # 1つ目は左下、2つ目は右下など、異なる方向にオフセット\n",
        "            if i == 0:\n",
        "                # 1つ目のインスタンス: 左上方向にオフセット\n",
        "                extremes_df_adjusted.loc[idx, 'Longitude'] -= 11.0\n",
        "                extremes_df_adjusted.loc[idx, 'Latitude'] += 5.0\n",
        "            elif i == 1:\n",
        "                # 2つ目のインスタンス: 左下方向にオフセット\n",
        "                extremes_df_adjusted.loc[idx, 'Longitude'] -= 11.0\n",
        "                extremes_df_adjusted.loc[idx, 'Latitude'] -= 5.0\n",
        "            else:\n",
        "                # 3つ目以降（もしあれば）: 下方向に順次オフセット\n",
        "                extremes_df_adjusted.loc[idx, 'Longitude'] += (i - 1) * 10.0\n",
        "                extremes_df_adjusted.loc[idx, 'Latitude'] -= 5.0\n",
        "\n",
        "    # 韓国のインスタンスを特定\n",
        "    korea_instances = extremes_df_adjusted[extremes_df_adjusted['Country'] == 'Korea']\n",
        "\n",
        "    # 韓国のインスタンスがある場合\n",
        "    if len(korea_instances) > 0:\n",
        "        print(f\"韓国のインスタンス ({len(korea_instances)}件) のラベル位置を調整します。\")\n",
        "\n",
        "        # 韓国のインスタンスのラベルを右下方向にオフセット\n",
        "        for idx in korea_instances.index:\n",
        "            extremes_df_adjusted.loc[idx, 'Longitude'] += 10.0\n",
        "            extremes_df_adjusted.loc[idx, 'Latitude'] -= 6.0\n",
        "\n",
        "    # 世界地図の作成\n",
        "    plt.figure(figsize=(14, 10))\n",
        "    ax = plt.axes(projection=ccrs.PlateCarree())\n",
        "\n",
        "    # 地図の基本要素を追加\n",
        "    ax.add_feature(cfeature.LAND, facecolor='lightgray')\n",
        "    ax.add_feature(cfeature.OCEAN, facecolor='lightblue')\n",
        "    ax.add_feature(cfeature.COASTLINE, linewidth=0.5)\n",
        "    ax.add_feature(cfeature.BORDERS, linewidth=0.5)\n",
        "    ax.gridlines(draw_labels=True, linewidth=0.5, color='gray', alpha=0.5, linestyle='--')\n",
        "    ax.set_global()\n",
        "\n",
        "    # カラーマップの作成\n",
        "    cmap = mpl.colors.LinearSegmentedColormap.from_list(\n",
        "        'custom_diverging',\n",
        "        [(0, 'darkblue'), (0.5, 'white'), (1, 'darkred')],\n",
        "        N=256\n",
        "    )\n",
        "\n",
        "    # 値の範囲を-0.5から2.5に設定\n",
        "    norm = mpl.colors.Normalize(vmin=-0.5, vmax=2.5)\n",
        "\n",
        "    # 極値インスタンスをプロット\n",
        "    marker_size = 20  # 統一サイズ\n",
        "\n",
        "    for _, row in extremes_df.iterrows():\n",
        "        # 元の位置にマーカーをプロット\n",
        "        # 予測値に基づいて色を決定\n",
        "        color = cmap(norm(row['PredictedValue']))\n",
        "\n",
        "        # マーカープロット\n",
        "        ax.scatter(row['Longitude'], row['Latitude'], s=marker_size, color=color, alpha=0.8,\n",
        "                  transform=ccrs.PlateCarree(), edgecolor='black', linewidth=0.5)\n",
        "\n",
        "    # 調整した位置にラベルを表示\n",
        "    for i, row in extremes_df_adjusted.iterrows():\n",
        "        # ラベルテキストの設定\n",
        "        label_text = f\"{row['Country']}\\n({row['PredictedValue']:.2f})\"\n",
        "\n",
        "        # ラベルを追加\n",
        "        ax.text(row['Longitude'], row['Latitude']+2, label_text,\n",
        "                fontsize=9, ha='center', va='bottom', transform=ccrs.PlateCarree(),\n",
        "                bbox=dict(facecolor='white', alpha=0.7, boxstyle='round,pad=0.2'))\n",
        "\n",
        "    # カラーバー\n",
        "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=norm)\n",
        "    sm.set_array([])\n",
        "\n",
        "    cbar = plt.colorbar(sm, ax=ax, shrink=0.6, pad=0.02)\n",
        "    cbar.set_label('Predicted Yield Difference')\n",
        "    # -0.5, 0, 1, 2, 2.5のティック位置にラベルを設定\n",
        "    cbar.set_ticks([-0.5, 0, 1, 2, 2.5])\n",
        "    cbar.set_ticklabels(['-0.5', '0', '1', '2', '2.5'])\n",
        "\n",
        "    plt.title('Top 3 Highest and Top 3 Lowest Predicted Yield Differences\\n(By Country, No Duplicates Within Groups)', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join('section4_predictions_analysis', 'extreme_values', 'Extreme_Values_Map.png'), dpi=300, bbox_inches='tight')\n",
        "    plt.close()\n",
        "\n",
        "    print(\"\\n調整した極値インスタンスの地図を 'section4_predictions_analysis/extreme_values/Extreme_Values_Map.png' に保存しました。\")\n",
        "\n",
        "    # ======= 4.3 選択した6個のインスタンスのWaterfall Plotを作成 =======\n",
        "\n",
        "    print(\"\\n3. 選択した6個の極値インスタンスのWaterfall Plotを作成中...\")\n",
        "\n",
        "    # 関数を定義：Waterfall Plotの作成\n",
        "    def create_extreme_waterfall_plot(idx, country, rank_info, X_processed, X_original, shap_values, explainer):\n",
        "        \"\"\"\n",
        "        極値インスタンスのWaterfall Plotを作成する関数\n",
        "        \"\"\"\n",
        "        if idx in X_processed.index:\n",
        "            try:\n",
        "                # データポイントの取得\n",
        "                instance_processed = X_processed.loc[idx:idx]\n",
        "                instance_original = X_original.loc[idx:idx] if idx in X_original.index else None\n",
        "\n",
        "                # インデックスの位置を取得\n",
        "                idx_pos = X_processed.index.get_loc(idx)\n",
        "\n",
        "                # SHAPウォーターフォールプロット\n",
        "                plt.figure(figsize=(12, 10))\n",
        "                waterfall = shap.waterfall_plot(\n",
        "                    shap.Explanation(\n",
        "                        values=shap_values[idx_pos],\n",
        "                        base_values=explainer.expected_value,\n",
        "                        data=instance_processed.iloc[0],\n",
        "                        feature_names=X_processed.columns\n",
        "                    ),\n",
        "                    show=False\n",
        "                )\n",
        "\n",
        "                # プロットのタイトル情報\n",
        "                pred_value = final_rf_model.predict(instance_processed)[0]\n",
        "                actual_value = y.loc[idx] if idx in y.index else np.nan\n",
        "\n",
        "                title = f'SHAP Waterfall Plot: {country} ({rank_info})\\n'\n",
        "                title += f'Predicted: {pred_value:.2f}, Actual: {actual_value:.2f}\\n'\n",
        "\n",
        "                # 補間前の特徴量値があれば追加\n",
        "                if instance_original is not None:\n",
        "                    # インスタンスの主要な特徴を取得\n",
        "                    feature_values = instance_original.iloc[0].to_dict()\n",
        "\n",
        "                    # 重要な特徴量に絞って表示（すべてだと多すぎる）\n",
        "                    # SHAP値の絶対値でソートして上位10個の特徴量のみ表示\n",
        "                    shap_abs = np.abs(shap_values[idx_pos])\n",
        "                    top_features_idx = np.argsort(shap_abs)[-10:][::-1]  # 上位10個\n",
        "                    top_features = [X_processed.columns[i] for i in top_features_idx]\n",
        "\n",
        "                    title += 'Top 10 feature values (before imputation):\\n'\n",
        "                    for i, feature in enumerate(top_features):\n",
        "                        if i > 0 and i % 2 == 0:  # 2つごとに改行\n",
        "                            title += '\\n'\n",
        "                        # NaN値の適切な表示\n",
        "                        value = feature_values.get(feature, np.nan)\n",
        "                        if pd.isna(value):\n",
        "                            title += f'{feature}: NaN, '\n",
        "                        else:\n",
        "                            title += f'{feature}: {value:.2f}, '\n",
        "\n",
        "                plt.title(title, fontsize=10)\n",
        "                plt.tight_layout()\n",
        "                plt.savefig(os.path.join('section4_predictions_analysis', 'extreme_values', f'SHAP_Waterfall_{country}_{rank_info.replace(\" \", \"_\")}.png'), bbox_inches='tight')\n",
        "                plt.close()\n",
        "\n",
        "                print(f\"{country} ({rank_info}) のWaterfall Plotを作成しました。\")\n",
        "                return True\n",
        "            except Exception as e:\n",
        "                print(f\"{country} ({rank_info}) のWaterfall Plot作成中にエラー: {e}\")\n",
        "                return False\n",
        "        else:\n",
        "            print(f\"警告: インデックス {idx} は処理済みデータに存在しません。\")\n",
        "            return False\n",
        "\n",
        "    # 元の特徴量データを取得\n",
        "    X_original = rice_data[X_processed.columns].copy() if X_processed.columns.tolist() else None\n",
        "    if X_original is not None:\n",
        "        # インデックスを合わせる\n",
        "        X_original = X_original.loc[X_original.index.isin(X_processed.index)]\n",
        "\n",
        "    # 上位3件のWaterfall Plotを作成\n",
        "    for i, row in enumerate(top_3_instances, 1):\n",
        "        idx = row['Index']\n",
        "        country = row['Country']\n",
        "        rank_info = f\"Top {i} - Highest\"\n",
        "        create_extreme_waterfall_plot(idx, country, rank_info, X_processed, X_original, shap_values, explainer)\n",
        "\n",
        "    # 下位3件のWaterfall Plotを作成\n",
        "    for i, row in enumerate(bottom_3_instances, 1):\n",
        "        idx = row['Index']\n",
        "        country = row['Country']\n",
        "        rank_info = f\"Bottom {i} - Lowest\"\n",
        "        create_extreme_waterfall_plot(idx, country, rank_info, X_processed, X_original, shap_values, explainer)\n",
        "\n",
        "    # Waterfall Plotをまとめた要約ページを作成\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    plt.text(0.5, 0.98, 'Waterfall Plots Summary: Extreme Values', horizontalalignment='center', fontsize=16, fontweight='bold')\n",
        "    plt.text(0.5, 0.95, 'Analysis of top 3 highest and top 3 lowest predicted yield differences', horizontalalignment='center', fontsize=12)\n",
        "\n",
        "    # 上位3国リスト\n",
        "    plt.text(0.05, 0.9, 'Top 3 Highest Predicted Yield Differences:', fontsize=11, fontweight='bold')\n",
        "    for i, row in enumerate(top_3_instances, 1):\n",
        "        plt.text(0.1, 0.88 - i*0.02, f\"{i}. {row['Country']}: {row['PredictedValue']:.4f}\", fontsize=10)\n",
        "\n",
        "    # 下位3国リスト\n",
        "    plt.text(0.05, 0.78, 'Top 3 Lowest Predicted Yield Differences:', fontsize=11, fontweight='bold')\n",
        "    for i, row in enumerate(bottom_3_instances, 1):\n",
        "        plt.text(0.1, 0.76 - i*0.02, f\"{i}. {row['Country']}: {row['PredictedValue']:.4f}\", fontsize=10)\n",
        "\n",
        "    plt.text(0.05, 0.65, 'Each waterfall plot shows how feature values contributed to model prediction,\\nwith both original (pre-imputation) and imputed values.', fontsize=10)\n",
        "    plt.axis('off')\n",
        "    plt.savefig(os.path.join('section4_predictions_analysis', 'extreme_values', 'Extreme_Values_Waterfall_Summary.png'))\n",
        "    plt.close()\n",
        "\n",
        "    print(\"\\nすべての極値インスタンスのWaterfall Plotが作成されました。\")\n",
        "    print(\"要約ページを 'section4_predictions_analysis/extreme_values/Extreme_Values_Waterfall_Summary.png' に保存しました。\")\n",
        "else:\n",
        "    print(\"インスタンスデータが見つかりません。Waterfall Plotは作成されません。\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "id": "rhVr7YdpCDrf",
        "outputId": "ee02d6e1-1c34-4bf9-8413-8cc572c9c144"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "SHAP値による解釈を開始...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'shap' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-647adebced60>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;31m# SHAP値の計算\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mexplainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTreeExplainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfinal_rf_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mshap_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexplainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshap_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_processed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'shap' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "全てのサンプルのSHAP値を1つの図にまとめ、各変数の重要度を可視化"
      ],
      "metadata": {
        "id": "V1KqM6WbvFJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============= ５．Beeswarm plotの作成 =============\n",
        "\n",
        "# Beeswarm Plot\n",
        "plt.figure(figsize=(12, 8))\n",
        "shap.summary_plot(shap_values, X_processed, plot_type=\"dot\", show=False)\n",
        "plt.title('SHAP Beeswarm Plot')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join('section5_shap_analysis', 'SHAP_Beeswarm.png'))\n",
        "plt.close()\n",
        "\n",
        "# 変数重要度（SHAP値の絶対値平均）\n",
        "feature_importance = pd.DataFrame({\n",
        "    'Feature': X_processed.columns,\n",
        "    'Importance': np.abs(shap_values).mean(axis=0)\n",
        "})\n",
        "feature_importance = feature_importance.sort_values('Importance', ascending=False)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "sns.barplot(x='Importance', y='Feature', data=feature_importance)\n",
        "plt.title('Feature Importance Based on SHAP Values')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join('section5_shap_analysis', 'SHAP_Feature_Importance.png'))\n",
        "plt.close()"
      ],
      "metadata": {
        "id": "8vqoUap4ue4g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SHAP値を用いた任意の2つの特徴量の相互作用の強さの定量化と、その予測値への影響の可視化"
      ],
      "metadata": {
        "id": "KssgDZonu6_g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============= ６．相互作用の解析 =============\n",
        "\n",
        "# 相互作用の分析\n",
        "print(\"\\n相互作用の分析を開始...\")\n",
        "\n",
        "# SHAP Interaction Values\n",
        "interaction_values = explainer.shap_interaction_values(X_processed)\n",
        "\n",
        "# 相互作用の平均絶対値を計算\n",
        "interaction_strength = np.zeros((X_processed.shape[1], X_processed.shape[1]))\n",
        "for i in range(X_processed.shape[1]):\n",
        "    for j in range(X_processed.shape[1]):\n",
        "        interaction_strength[i, j] = np.abs(interaction_values[:, i, j]).mean()\n",
        "\n",
        "# 相互作用の強さ（対角成分を除く）\n",
        "interaction_df = []\n",
        "for i in range(X_processed.shape[1]):\n",
        "    for j in range(i+1, X_processed.shape[1]):\n",
        "        interaction_df.append({\n",
        "            'Feature1': X_processed.columns[i],\n",
        "            'Feature2': X_processed.columns[j],\n",
        "            'Interaction_Strength': interaction_strength[i, j] + interaction_strength[j, i]\n",
        "        })\n",
        "interaction_df = pd.DataFrame(interaction_df)\n",
        "interaction_df = interaction_df.sort_values('Interaction_Strength', ascending=False)\n",
        "\n",
        "# 上位10の相互作用を可視化\n",
        "plt.figure(figsize=(14, 8))\n",
        "top_interactions = interaction_df.head(10)\n",
        "sns.barplot(x='Interaction_Strength', y=top_interactions.apply(lambda x: f\"{x['Feature1']} × {x['Feature2']}\", axis=1), data=top_interactions)\n",
        "plt.title('Top 10 Feature Interactions')\n",
        "plt.tight_layout()\n",
        "plt.savefig(os.path.join('section6_interaction_analysis', 'SHAP_Top_Interactions.png'))\n",
        "plt.close()\n",
        "\n",
        "# 上位3つの相互作用に対するPartial Dependence Plots\n",
        "print(\"\\n上位3つの相互作用のPartial Dependence Plotを作成...\")\n",
        "\n",
        "# PDPの計算と可視化のための関数\n",
        "def create_pdp_interaction(model, X, feature1, feature2, grid_resolution=20):\n",
        "    # 特徴量の範囲を決定\n",
        "    f1_min, f1_max = X[feature1].min(), X[feature1].max()\n",
        "    f2_min, f2_max = X[feature2].min(), X[feature2].max()\n",
        "\n",
        "    # グリッドポイントを作成\n",
        "    f1_grid = np.linspace(f1_min, f1_max, grid_resolution)\n",
        "    f2_grid = np.linspace(f2_min, f2_max, grid_resolution)\n",
        "\n",
        "    # PDPの計算\n",
        "    pdp_values = np.zeros((grid_resolution, grid_resolution))\n",
        "\n",
        "    for i, v1 in enumerate(f1_grid):\n",
        "        for j, v2 in enumerate(f2_grid):\n",
        "            X_temp = X.copy()\n",
        "            X_temp[feature1] = v1\n",
        "            X_temp[feature2] = v2\n",
        "            # 予測を計算\n",
        "            predictions = model.predict(X_temp)\n",
        "            pdp_values[i, j] = predictions.mean()\n",
        "\n",
        "    # 結果をプロット\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    plt.contourf(f1_grid, f2_grid, pdp_values.T, levels=50, cmap='viridis')\n",
        "    plt.colorbar(label='Predicted Yield Difference')\n",
        "    plt.xlabel(feature1)\n",
        "    plt.ylabel(feature2)\n",
        "    plt.title(f'Partial Dependence Plot: {feature1} × {feature2}')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join('section6_interaction_analysis', f'PDP_{feature1}_{feature2}.png'))\n",
        "    plt.close()\n",
        "\n",
        "# 上位3つの相互作用に対するPDPを作成\n",
        "for i in range(min(3, len(interaction_df))):\n",
        "    interaction = interaction_df.iloc[i]\n",
        "    feature1 = interaction['Feature1']\n",
        "    feature2 = interaction['Feature2']\n",
        "    print(f\"Creating PDP for interaction: {feature1} × {feature2}\")\n",
        "\n",
        "    try:\n",
        "        create_pdp_interaction(final_rf_model, X_processed, feature1, feature2)\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating PDP for {feature1} × {feature2}: {e}\")\n",
        "\n",
        "print(\"\\n相互作用の分析が完了しました。\")"
      ],
      "metadata": {
        "id": "-0IMzlbdunIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ある特徴量の変化によって、予測値がどう変わるか、の可視化"
      ],
      "metadata": {
        "id": "ImtOYCNmu0zT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============= ７．異なる特徴量による予測値の変化を調査 =============\n",
        "\n",
        "# 重要な特徴量を取得\n",
        "top_features = feature_importance.head(5)['Feature'].tolist()\n",
        "print(f\"\\n重要度上位5つの特徴量: {top_features}\")\n",
        "\n",
        "# 各特徴量の値が変化した場合の予測値の変化を調査\n",
        "def plot_single_feature_pdp(model, X, feature, grid_resolution=50):\n",
        "    # 特徴量の範囲を決定\n",
        "    f_min, f_max = X[feature].min(), X[feature].max()\n",
        "\n",
        "    # グリッドポイントを作成\n",
        "    f_grid = np.linspace(f_min, f_max, grid_resolution)\n",
        "\n",
        "    # PDPの計算\n",
        "    pdp_values = []\n",
        "\n",
        "    for val in f_grid:\n",
        "        X_temp = X.copy()\n",
        "        X_temp[feature] = val\n",
        "        # 予測を計算\n",
        "        predictions = model.predict(X_temp)\n",
        "        pdp_values.append(predictions.mean())\n",
        "\n",
        "    # 結果をプロット\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(f_grid, pdp_values, 'b-', linewidth=2)\n",
        "    plt.xlabel(feature)\n",
        "    plt.ylabel('Predicted Yield Difference')\n",
        "    plt.title(f'Partial Dependence Plot: {feature}')\n",
        "    plt.grid(True, alpha=0.3)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(os.path.join('section7_feature_pdp', f'PDP_Single_{feature}.png'))\n",
        "    plt.close()\n",
        "\n",
        "# 上位5つの重要な特徴量に対するPDPを作成\n",
        "for feature in top_features:\n",
        "    print(f\"Creating PDP for feature: {feature}\")\n",
        "    try:\n",
        "        plot_single_feature_pdp(final_rf_model, X_processed, feature)\n",
        "    except Exception as e:\n",
        "        print(f\"Error creating PDP for {feature}: {e}\")\n",
        "\n",
        "print(\"\\n単一特徴量のPDPの作成が完了しました。\")\n",
        "\n",
        "print(\"\\n分析が完了しました。結果をそれぞれのフォルダに保存しました。\")"
      ],
      "metadata": {
        "id": "kCsyhkYQupYB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "保存したフォルダを自分のPCにダウンロード"
      ],
      "metadata": {
        "id": "WGbJt8TRutNa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import zipfile\n",
        "from google.colab import files as colab_files  # 名前を変更してインポート\n",
        "\n",
        "def create_zip_and_download_fixed():\n",
        "    \"\"\"すべての分析結果フォルダをZIPアーカイブに圧縮してダウンロード（修正版）\"\"\"\n",
        "    # 分析セクションのフォルダリスト\n",
        "    sections = [\n",
        "        \"section1_data_preparation\",\n",
        "        \"section2_model_comparison\",\n",
        "        \"section3_final_model\",\n",
        "        \"section4_predictions_analysis\",\n",
        "        \"section5_shap_analysis\",\n",
        "        \"section6_interaction_analysis\",\n",
        "        \"section7_feature_pdp\"\n",
        "    ]\n",
        "\n",
        "    # 圧縮ファイル名\n",
        "    zip_filename = \"biochar_analysis_results.zip\"\n",
        "\n",
        "    # すでに存在するZIPファイルを削除\n",
        "    if os.path.exists(zip_filename):\n",
        "        os.remove(zip_filename)\n",
        "\n",
        "    # ZIPファイルを作成\n",
        "    with zipfile.ZipFile(zip_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:\n",
        "        # 各フォルダ内のファイルを追加\n",
        "        for section in sections:\n",
        "            if os.path.exists(section):\n",
        "                # フォルダ内のすべてのファイルとサブフォルダを取得\n",
        "                for root, dirs, file_list in os.walk(section):\n",
        "                    for file in file_list:\n",
        "                        # ファイルへのフルパス\n",
        "                        file_path = os.path.join(root, file)\n",
        "                        # ZIPファイル内のパス (相対パス)\n",
        "                        arcname = os.path.relpath(file_path)\n",
        "                        # ZIPに追加\n",
        "                        zipf.write(file_path, arcname)\n",
        "                        print(f\"Added to ZIP: {file_path}\")\n",
        "\n",
        "    # ZIPファイルをダウンロード (修正した部分)\n",
        "    colab_files.download(zip_filename)  # colab_filesを使用\n",
        "    print(f\"\\nダウンロード完了: {zip_filename}\")\n",
        "    print(\"すべての分析結果ファイルが含まれています。\")\n",
        "\n",
        "# 修正した関数を実行\n",
        "create_zip_and_download_fixed()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RCZD_z0LAoaF",
        "outputId": "21a6e08c-1bc7-4bd1-bd56-d93efb5b59b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added to ZIP: section1_data_preparation/Yield_Difference_Distribution_After_Filtering.png\n",
            "Added to ZIP: section1_data_preparation/histograms/hist_Texture_initial.png\n",
            "Added to ZIP: section1_data_preparation/histograms/hist_N_biochar.png\n",
            "Added to ZIP: section1_data_preparation/histograms/hist_SOC_initial.png\n",
            "Added to ZIP: section1_data_preparation/histograms/hist_Temperature_before_imputation.png\n",
            "Added to ZIP: section1_data_preparation/histograms/hist_Temperature.png\n",
            "Added to ZIP: section1_data_preparation/histograms/hist_P_biochar.png\n",
            "Added to ZIP: section1_data_preparation/histograms/hist_Texture_initial_before_imputation.png\n",
            "Added to ZIP: section1_data_preparation/histograms/hist_Type_biochar.png\n",
            "Added to ZIP: section1_data_preparation/histograms/hist_C_biochar.png\n",
            "Added to ZIP: section1_data_preparation/histograms/hist_pH_biochar.png\n",
            "Added to ZIP: section1_data_preparation/histograms/hist_PyrolysisTemperature_biochar_before_imputation.png\n",
            "Added to ZIP: section1_data_preparation/histograms/hist_TN_initial_before_imputation.png\n",
            "Added to ZIP: section1_data_preparation/histograms/hist_TrialDuration.png\n",
            "Added to ZIP: section1_data_preparation/histograms/hist_pH_biochar_before_imputation.png\n",
            "Added to ZIP: section1_data_preparation/histograms/hist_BiocharAddition.png\n",
            "Added to ZIP: section1_data_preparation/histograms/hist_CNRatio_biochar.png\n",
            "Added to ZIP: section1_data_preparation/histograms/hist_pH_initial_before_imputation.png\n",
            "Added to ZIP: section1_data_preparation/histograms/hist_Precipitation.png\n",
            "Added to ZIP: section1_data_preparation/histograms/hist_N_biochar_before_imputation.png\n",
            "Added to ZIP: section1_data_preparation/histograms/hist_pH_initial.png\n",
            "Added to ZIP: section1_data_preparation/histograms/hist_TrialDuration_before_imputation.png\n",
            "Added to ZIP: section1_data_preparation/histograms/hist_Type_biochar_before_imputation.png\n",
            "Added to ZIP: section1_data_preparation/histograms/hist_PyrolysisTemperature_biochar.png\n",
            "Added to ZIP: section1_data_preparation/histograms/hist_SOC_initial_before_imputation.png\n",
            "Added to ZIP: section1_data_preparation/histograms/hist_Precipitation_before_imputation.png\n",
            "Added to ZIP: section1_data_preparation/histograms/hist_Yield_Difference.png\n",
            "Added to ZIP: section1_data_preparation/histograms/hist_P_biochar_before_imputation.png\n",
            "Added to ZIP: section1_data_preparation/histograms/hist_CNRatio_biochar_before_imputation.png\n",
            "Added to ZIP: section1_data_preparation/histograms/hist_C_biochar_before_imputation.png\n",
            "Added to ZIP: section1_data_preparation/histograms/hist_TN_initial.png\n",
            "Added to ZIP: section1_data_preparation/histograms/hist_BiocharAddition_before_imputation.png\n",
            "Added to ZIP: section2_model_comparison/PLSR_cv_scatter_simple.png\n",
            "Added to ZIP: section2_model_comparison/Decision Tree_cv_scatter_simple.png\n",
            "Added to ZIP: section2_model_comparison/algorithm_comparison_cv_r2_simple.png\n",
            "Added to ZIP: section2_model_comparison/Neural Network_cv_scatter_simple.png\n",
            "Added to ZIP: section2_model_comparison/Random Forest_cv_scatter_simple.png\n",
            "Added to ZIP: section2_model_comparison/cross_validation_results_simple.xlsx\n",
            "Added to ZIP: section2_model_comparison/algorithm_comparison_cv_rrmse_simple.png\n",
            "Added to ZIP: section2_model_comparison/algorithm_comparison_cv_rmse_simple.png\n",
            "Added to ZIP: section2_model_comparison/XGBoost_cv_scatter_simple.png\n",
            "Added to ZIP: section3_final_model/RandomForest_Final_scatter.png\n",
            "Added to ZIP: section4_predictions_analysis/All_Instances_Prediction_Map.png\n",
            "Added to ZIP: section4_predictions_analysis/All_Instances_Predictions.csv\n",
            "Added to ZIP: section4_predictions_analysis/extreme_values/SHAP_Waterfall_China_Top_2_-_Highest.png\n",
            "Added to ZIP: section4_predictions_analysis/extreme_values/Extreme_Values_Map.png\n",
            "Added to ZIP: section4_predictions_analysis/extreme_values/Extreme_Values_Waterfall_Summary.png\n",
            "Added to ZIP: section4_predictions_analysis/extreme_values/SHAP_Waterfall_Iran_Top_1_-_Highest.png\n",
            "Added to ZIP: section4_predictions_analysis/extreme_values/SHAP_Waterfall_India_Top_3_-_Highest.png\n",
            "Added to ZIP: section4_predictions_analysis/extreme_values/SHAP_Waterfall_China_Bottom_1_-_Lowest.png\n",
            "Added to ZIP: section4_predictions_analysis/extreme_values/SHAP_Waterfall_Korea_Bottom_2_-_Lowest.png\n",
            "Added to ZIP: section4_predictions_analysis/extreme_values/SHAP_Waterfall_Philippines_Bottom_3_-_Lowest.png\n",
            "Added to ZIP: section5_shap_analysis/SHAP_Beeswarm.png\n",
            "Added to ZIP: section5_shap_analysis/SHAP_Feature_Importance.png\n",
            "Added to ZIP: section6_interaction_analysis/SHAP_Top_Interactions.png\n",
            "Added to ZIP: section6_interaction_analysis/PDP_pH_initial_C_biochar.png\n",
            "Added to ZIP: section6_interaction_analysis/PDP_C_biochar_CNRatio_biochar.png\n",
            "Added to ZIP: section6_interaction_analysis/PDP_pH_initial_CNRatio_biochar.png\n",
            "Added to ZIP: section7_feature_pdp/PDP_Single_TrialDuration.png\n",
            "Added to ZIP: section7_feature_pdp/PDP_Single_CNRatio_biochar.png\n",
            "Added to ZIP: section7_feature_pdp/PDP_Single_C_biochar.png\n",
            "Added to ZIP: section7_feature_pdp/PDP_Single_pH_initial.png\n",
            "Added to ZIP: section7_feature_pdp/PDP_Single_Precipitation.png\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_c26a49f4-0654-4e26-8f38-c8c378152e9e\", \"biochar_analysis_results.zip\", 3713887)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ダウンロード完了: biochar_analysis_results.zip\n",
            "すべての分析結果ファイルが含まれています。\n"
          ]
        }
      ]
    }
  ]
}